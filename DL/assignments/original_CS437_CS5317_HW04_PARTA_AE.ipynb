{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"CS437_CS5317_HW04_PARTA_AE.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qhSkwMQ3ivrR"},"source":["### IMAGE COMPLETION\n","\n","Image Completion is the task of filling missing parts of a given image with the help of information from the known parts of the image. This is an application that takes an image with a missing part as input and gives a completed image as the result.\n","\n","We will be using Autoencoder to do this task. We will train our network on Images with missing parts passed with true images so that autoencoder can minimize the ture image and corrupted image. "]},{"cell_type":"markdown","metadata":{"id":"b_zdyM1bKvHu"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"SFJ9ltxCivrS"},"source":["import os\n","import random\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","\n","import tensorflow as tf\n","from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n","from keras.layers import Activation, BatchNormalization, LeakyReLU, Dropout\n","from keras.models import Model\n","from keras import backend as K\n","from keras.optimizers import Adam\n","from keras.utils import plot_model\n","from skimage.io import imread\n","from PIL import Image, ImageDraw"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2niHiRKMmvI"},"source":["### Data\n","\n","The data we are using is [Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset). It is an unlabelled dataset used for training GANs and other image generation algorithms. The original dataset has images of size 1024 by 1024 but we have only taken 128 by 128 images. "]},{"cell_type":"markdown","metadata":{"id":"b21_0FHGsY-S"},"source":["Mounting your google drive."]},{"cell_type":"code","metadata":{"id":"iMOtjPOsM3vg"},"source":["from google.colab import drive\r\n","drive.mount('/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NeafZrkgsgBs"},"source":["Unzipping the data file to load it locally in the colab runtime. You can see your unzipped files by clicking the folder icon on left side of your colab."]},{"cell_type":"code","metadata":{"id":"NOeX1b3k2NfY"},"source":["# replace this your google drive path of the zip file of dataset provided with this homework\r\n","!unzip -o -q \"/drive/MyDrive/CS5317_DeepLearning_SP21/Assign04/ffhq-dataset.zip\" -d \"/content/data/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1ur9Fq2OG3v"},"source":["# data folder path in colab runtime enviroment\n","DATA_FOLDER = '/content/data/'\n","\n","# fetching all the filenmaes to read them later in generator\n","filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.png')))\n","\n","# total images in directory\n","NUM_IMAGES = len(filenames)\n","print(\"Total number of images : \" + str(NUM_IMAGES))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAOEfLF0Oi4N"},"source":["### Data Generator\n","\n","The dataset is quite large (70000 images) which makes it impossible to load it all at the same time in computer memory. In you assignment 03 you implemented a custom generator function to load the images in batches, here we are going to do the same. Instead of returning the images and its labels, here we will return tuple <i>(corrupted_images_batch, original_images_batch)</i> from the generator where the corrupted images are the same images as the original but a small square is removed from them."]},{"cell_type":"markdown","metadata":{"id":"XLqBbG-mNiQB"},"source":["Below you will create a function to remove a portion of image. This is basically the same as drawing a black square on the image. You function will take a numpy image and return the numpy image with black square on it. \n","\n","\n","You images will look somehting like this. The square drawn here is 28x28 (you can be confortable with the dimensions) and it is drawn at a random location with-in the image.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1qIlWIj1K_qjxoGTUJYeVbiLztnyMOVP_)\n","\n","\n","<i>HINT: You can use ImageDraw function of PIL</i>"]},{"cell_type":"code","metadata":{"id":"eCJ5qk-uvLCD"},"source":["def draw_square_on_image(image):\n","    \n","    output_img = None\n","    \n","    ######################## WRITE YOUR CODE BELOW ########################\n","\n","    ########################### END OF YOUR CODE ##########################\n","\n","    return np.array(output_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcyfNQNEUjW2"},"source":["If you are not fimiliar with generators in python you can a look at it [here](https://realpython.com/introduction-to-python-generators/).\n","\n","As mentioned above, below generator will return a tuple of <i>(corrupted_images_batch, original_images_batch)</i>."]},{"cell_type":"code","metadata":{"id":"wtc5Zu89pDEp"},"source":["def custom_image_generator(files, data_instances, batch_size = 64):\n","\n","    ######################## WRITE YOUR CODE BELOW ########################\n","\n","    # to keep track that you don't have invalid index for number of files\n","    iter = 0\n","\n","    while True:\n","\n","        # check if you have a invalid index for files, if yes then reset it\n","\n","\n","        # Select files (paths/indices) for the batch\n","\n","         \n","        # Read in each input and perform preprocessing (to batch of images)\n","\n","\n","        # Return a tuple of (corrupted_image_batch, true_image_batch) to feed the network\n","        corrupted_images_batch = np.array(batch_input)\n","        original_images_batch = np.array(proc_batch)\n","\n","        # move to the next batch\n","        iter = iter + 1\n","\n","    ########################### END OF YOUR CODE ##########################\n","    \n","        yield (corrupted_images_batch, original_images_batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"748HAEQnw7qO"},"source":["Utility function to display grid of images."]},{"cell_type":"code","metadata":{"id":"w-hh7h1Pt6q9"},"source":["def display_image_grid(images, num_rows, num_cols, title_text):\n","\n","    fig = plt.figure(figsize=(num_cols*3., num_rows*3.), )\n","    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.15)\n","\n","    for ax, im in zip(grid, images):\n","        ax.imshow(im)\n","        ax.axis(\"off\")\n","    \n","    plt.suptitle(title_text, fontsize=20)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhvVhrO0pS1y"},"source":["# create generator object\n","test_generator = custom_image_generator(filenames, 7000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QD14P1V0WxT_"},"source":["Displaying sample images from batch generator."]},{"cell_type":"code","metadata":{"id":"GXgRN-0eqdDg"},"source":["# get first batch of images\n","(corrupted_images_batch, orig_images_batch) = next(test_generator)\n","\n","# only displaying 10 images from both batch\n","display_image_grid(orig_images_batch[:10], 2, 5, \"Original Images\")\n","display_image_grid(corrupted_images_batch[:10], 2, 5, \"Corrupted Images\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-I53l3rAPqyE"},"source":["INPUT_DIM = (128,128,3) # Image dimension\n","Z_DIM = 300             # Dimension of the latent vector (z)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IHNXkqbQSPab"},"source":["## Building the Model"]},{"cell_type":"markdown","metadata":{"id":"bZhwpjUmgDxq"},"source":["#### Encoder\n","\n","Below you will create the model for the encoder. The architecture of the Encoder consists of a stack of convolutional layers followed by a dense (fully connected) layer which outputs a vector of size <b>Z_DIM</b>. The whole image of size 128x128x3 is decoed into this latent space vector of size <b>Z_DIM</b>.\n","\n","for i=1 to num_conv:\n","  - add Covn Layer (filter_size = 32, stride = 2, padding = 'same')\n","  - add LeakeyReLU\n","\n","end\n","- add Dense() (with no activation function)\n","\n","NOTE: You can also experiment with the number of feature maps, kernel size and strides for each of the conv layer.\n","\n","You can refer to this [link](https://blog.keras.io/building-autoencoders-in-keras.html) to see how to create autoencoer model in keras."]},{"cell_type":"code","metadata":{"id":"8XlsuP_NeNWb"},"source":["ae_encoder = None\n","ae_encoder_output = None\n","ae_encoder_input = None\n","\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","ae_encoder.summary()\n","# tf.keras.utils.plot_model(ae_encoder, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYC79JgkjCYO"},"source":["#### Decoder\n","\n","Just like the encoder you will create the model for the decoder. This model will be the exact mirror of encoder model, but that is not mandatory.\n","\n","Since the function of the Decoder to reconstruct the image from the latent vector. Therefore, it is necessary to define the decoder so as to increase the size of the activations gradually through the network. This can be achieved through the  [Conv2DTransponse](https://keras.io/layers/convolutional/#conv2dtranspose) layer. This layer produces an output tensor double the size of the input tensor in both height and width. The input to the encoder is the vector of size <b>Z_DIM</b> and output will be a image of size <b>INPUT_DIM</b>. Your final decoder will look something like this: \n","\n","<center>\n","\n","![picture](https://drive.google.com/uc?export=view&id=1QGaPm7byp7YOZqrZx9ARX9hPBVHB9rq5)\n","\n","</center>\n","\n","Again, you can experiment with the number of layers, feature size, kernel size and stride of conv layers.\n","\n","<i>NOTE: Unlike the encoder, there will the activaiton function for decoder, as it will be outputing the image. And we want our pixel values between zero and one. </i>\n"]},{"cell_type":"code","metadata":{"id":"TvBVuQvMjF3A"},"source":["ae_decoder = None\n","ae_decoder_output = None\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","ae_decoder.summary()\n","# tf.keras.utils.plot_model(ae_decoder, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-lK7PJGCyoa9"},"source":["#### Attaching the Decoder to the Encoder\n","\n","Finally, here we connect the encoder to the docoder."]},{"cell_type":"code","metadata":{"id":"ma7ZQ2pow2am"},"source":["autoencoder_model = None\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","# The input of the autoencoder will be the same as of encoder\n","\n","\n","# The output of the autoencoder will be the output of decoder, when passed encoder input\n","\n","\n","# Input to the combined model will be the input to the encoder.\n","# Output of the combined model will be the output of the decoder.\n","\n","########################### END OF YOUR CODE ##########################\n","\n","autoencoder_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29JoCbVi5PJO"},"source":["## Training the AE\n","\n","Finally you will compile your autoencoer here. \n","\n","Here are few hyperparamters to consider here:\n","- Learning rate [0.1, 0.00001]\n","- Training epochs [5, 50]\n","- batch_size [64, 512]\n","- Latent vector size [20, 5000]\n","- Error function \n","- Optimizer"]},{"cell_type":"code","metadata":{"id":"JdGJw4Fb6I7e"},"source":["LEARNING_RATE = None    # learning rate\n","N_EPOCHS = None         # epochs\n","BATCH_SIZE = None       # batch of images returned by image generator\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","# create custom_data_generator here\n","\n","\n","# compile your model here\n","\n","########################### END OF YOUR CODE ##########################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mJeKcn57JQA"},"source":["Now simply call the <i>fit</i> function of the model with the appropriate paramters.\n","\n","<i> HINT: Pass step_per_epoch to be equal to total images divided by batch_size. As you want to see all your data in a single epoch. </i>"]},{"cell_type":"code","metadata":{"id":"1F7g3wxz6Mkp"},"source":["######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbFnCBjKu9xa"},"source":["## Reconstruction\n","\n","Now we will get a batch of images from data generator object and try to reproduce it by passing through AE.\n","\n","The first image grid shows the original images and the second grid shows the reconstructed images after passing it through the AE."]},{"cell_type":"code","metadata":{"id":"ft5zuMQ6CpyA"},"source":["test_gen = custom_image_generator(filenames, NUM_IMAGES)\n","\n","test_batch = next(test_gen)[0]\n","test_images = test_batch[:10]\n","\n","reconst_images = autoencoder_model.predict(test_images)\n","\n","display_image_grid(test_images, 2, 5, \"Incomplete Images\")\n","display_image_grid(reconst_images, 2, 5, \"Reconstructed Images with Autoencoder\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ju15fCLgt5R"},"source":["## REPORT\n","\n","Report your results for different values of <b>Z_DIM</b>, <b>learning rate</b>, <b>optimizers</b>, <b> encoder and decoder model</b> and tell us for which configuration you acheived the best results (The best run model should be the last run model in this notebook, showing the results in the cell above)."]},{"cell_type":"markdown","metadata":{"id":"CaFCQ5IkjgL4"},"source":["Your answer:"]}]}