{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VGG of CS437_HW3_PartC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlM2iatsQ9nb"
      },
      "source": [
        "# Assignment 3  - Part C\r\n",
        "\r\n",
        "\r\n",
        "#### Roll Number:20030012\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_kQzTmZRTLu"
      },
      "source": [
        "### Task Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m73XPqSORiIJ"
      },
      "source": [
        "In this task, we will perform feature extraction and fine-tuning on a previously trained over a large dataset model. This is a very useful domain of Deep Learning known as Transfer Learning. We don't always have the luxury of time and computational power to adequately train a large model over our dataset. So, we use models which have been trained on large datasets (usually ImageNet) and then fine-tune them on our dataset. The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\r\n",
        "\r\n",
        "Here, we will fine-tune a previously trained model on imagenet to learn and classify three new class: Richshaw, qingqi and Tanga. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5dOkrpMXxwA"
      },
      "source": [
        "### Dataset\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mbwUfb_YV07"
      },
      "source": [
        "The dataset is divided into training set and testing set. The training set is labeled and contains 80 images for each class (Total 240 images). You have been given unlabeled test set and your task is to predict the class of each test image. The test set contain 853 images.<br>\r\n",
        "Some guidelines:<br>\r\n",
        "- You are free to extend or augment the training set.<br>\r\n",
        "- If you want, you can use a subset of training set as your validation set.<br>\r\n",
        "- You must not use the provided test set in any way except for final predictions.<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTAWM3gkYCFA",
        "outputId": "7b27e38e-b54f-4b34-be2b-0c07ba585dc9"
      },
      "source": [
        "!git clone https://github.com/MMFa666/VehicleDataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'VehicleDataset'...\n",
            "remote: Enumerating objects: 1106, done.\u001b[K\n",
            "remote: Counting objects: 100% (1106/1106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1100/1100), done.\u001b[K\n",
            "remote: Total 1106 (delta 4), reused 1103 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1106/1106), 34.18 MiB | 42.84 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuuKM6L7XvVB"
      },
      "source": [
        "### Models\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxOeWlzGYZcw"
      },
      "source": [
        "You can use anyone of the following models for this task. See the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications#functions) for more details.\r\n",
        "- VGG\r\n",
        "- ResNet\r\n",
        "- MobileNet\r\n",
        "- EfficientNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgf-tLf1YRKV"
      },
      "source": [
        "### LeaderBoard\r\n",
        "\r\n",
        "The goal here is to get the maximum performance on test set. For this purpose, we have created a competition on Kaggle where a leaderboard is maintained. You will upload your predictions for test set there and a score will be generated for you. You will be ranked based on your score. You can submit upto 20 times a day with maximum 300 submissions in total.\r\n",
        "\r\n",
        "This part will be evaluated based on your approach and your performance in LeaderBoard. Furthermore, the top student will receive a 5%, 2nd will receive 3% and 3rd will receive 1% bonus.\r\n",
        "\r\n",
        "You are allowed to the following to win:\r\n",
        "- Extend your chosen model as you like.\r\n",
        "- Extend the training set.\r\n",
        "- Augment the training set.\r\n",
        "- Play with hyperparameters.\r\n",
        "- Preprocess the data as you like.\r\n",
        "\r\n",
        "Competition link [here](https://www.kaggle.com/c/lums-cs437-hw3/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfMOftyqbQbp"
      },
      "source": [
        "### Tutuorial Example\r\n",
        "\r\n",
        "You are recommended to follow this [tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning) for this task. All the hints and information required are available here and this task can be easily completed by following it. In this tutorial, they use MobileNet but as mentioned above you are free to use any of the abovementioned models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFkG-dn-cSH3"
      },
      "source": [
        "### Let The Games Begin!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_POqcmrGQloX"
      },
      "source": [
        "# make necessary imports here\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import random\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n",
        "import cv2\r\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, BatchNormalization, Input, LeakyReLU, Flatten, Dropout, GlobalAveragePooling2D, experimental,MaxPooling2D\r\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\r\n",
        "from tensorflow.keras import Model\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "# any other imports that you may require\r\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow import expand_dims\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from tensorflow.keras.applications import mobilenet_v2, MobileNetV2\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "from tensorflow.image import convert_image_dtype\r\n",
        "from tensorflow import float16\r\n",
        "import math\r\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_w0XI7FeBj_"
      },
      "source": [
        "labels={}\r\n",
        "labels['qingqi']=[1,0,0]\r\n",
        "labels['rickshaw']=[0,1,0]\r\n",
        "labels['tanga']=[0,0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FvcGUc6c4sB"
      },
      "source": [
        "# define hyperparameters\r\n",
        "\r\n",
        "epochs = 15  ## 20\r\n",
        "learning_rate = 0.0001  ## 0.0001\r\n",
        "batch_size = 30\r\n",
        "\r\n",
        "input_shape = (224,224,3)\r\n",
        "img_size=(224,224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw7YXkc5ZU59",
        "outputId": "36543857-87fc-4214-9e78-d8ea52330103"
      },
      "source": [
        "## augmenting the data set\n",
        "!rm -rf VehicleDataset/  ## clear directory first\n",
        "!git clone https://github.com/MMFa666/VehicleDataset.git ## clone dataset\n",
        "\n",
        "## now augement trainign set\n",
        "data_augmentation = Sequential([\n",
        "  experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "for dataset_folder in glob.glob('/content/VehicleDataset/train*'):\n",
        "  for type_folder in glob.glob(dataset_folder + '/*'):\n",
        "    for img_link in glob.glob(type_folder+'/*'):\n",
        "      img_name = img_link.split('/')[-1].split('.')[0] \n",
        "      img = cv2.imread(img_link)\n",
        "      img.resize((input_shape))\n",
        "      for k in range(10): ## for each img generate 3 versions rotated for train set\n",
        "        augmented_image = data_augmentation(expand_dims(img, 0))\n",
        "        cv2.imwrite(os.path.join(type_folder, img_name+'__'+str(k)+'_.jpg'), np.array(augmented_image[0]))\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'VehicleDataset'...\n",
            "remote: Enumerating objects: 1106, done.\u001b[K\n",
            "remote: Counting objects: 100% (1106/1106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1100/1100), done.\u001b[K\n",
            "remote: Total 1106 (delta 4), reused 1103 (delta 4), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1106/1106), 34.18 MiB | 35.53 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4Jcy6uUpA6H"
      },
      "source": [
        "# Batch generator function here.\n",
        "\n",
        "\n",
        "def data_generator(file_path):\n",
        "\n",
        "  batch_train_paths = [file_path[i:i+batch_size] for i in range(0,len(file_path),batch_size)]\n",
        "  # print(np.tile(batch_train_paths,epochs).shape\n",
        "  for i in range(math.ceil(train_examples/batch_size)*epochs):\n",
        "\n",
        "    for batch in batch_train_paths:\n",
        "      one_hot_labels = []\n",
        "      imgs_list = []\n",
        "      for path in batch:\n",
        "        img = cv2.imread(path)\n",
        "        img.resize((input_shape))\n",
        "        img= preprocess_input(img)\n",
        "        imgs_list.append(img)\n",
        "        imgs_label = path.split('/')[4] ## getting label name\n",
        "        one_hot_labels.append(labels[imgs_label])\n",
        "      yield np.array(imgs_list), np.array(one_hot_labels)\n",
        "\n",
        "\n",
        "train_paths = []\n",
        "for type_folder in glob.glob('/content/VehicleDataset/train/*'):\n",
        "  for img_link in glob.glob(type_folder + '/*'):\n",
        "    train_paths.append(img_link)\n",
        "train_examples = len(train_paths)\n",
        "\n",
        "    \n",
        "random.shuffle(train_paths)\n",
        "train_dataset = data_generator(train_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5oPAMnkTW5-"
      },
      "source": [
        "# read and preprocess the training data and extract the labels. (If your training data is large then you can also use batch generator like done in PartB.)\r\n",
        "# print the shape x and y data\r\n",
        "# split into validation set if you desire\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# train_dir = '/content/VehicleDataset/train/'\r\n",
        "# test_dir = '/content/VehicleDataset/test/'\r\n",
        "\r\n",
        "# train_dataset = image_dataset_from_directory(train_dir,\r\n",
        "#                                              shuffle=True,\r\n",
        "#                                              batch_size=batch_size,\r\n",
        "#                                              image_size=img_size,\r\n",
        "#                                              label_mode = 'categorical')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKxIz_Jbbh5c"
      },
      "source": [
        "# np.array((next(iter(train_dataset))[0])).max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERyMxckRznzg"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfrbq528z5WX"
      },
      "source": [
        "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj5LalB6fgwh",
        "outputId": "2fc4cee3-f08e-420e-a128-978c9a735283"
      },
      "source": [
        "# intialize the base model here and print the model summary\r\n",
        "\r\n",
        "\r\n",
        "base_model = VGG16(weights='imagenet', include_top=True)\r\n",
        "\r\n",
        "base_model.summary()\r\n",
        "\r\n",
        "# global_average_layer = GlobalAveragePooling2D()\r\n",
        "# preprocess_input = preprocess_input\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aQFbNFv01e3"
      },
      "source": [
        "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vuzbvanmPWU"
      },
      "source": [
        "# Freeze the base Model\r\n",
        "# base_model.trainable = False\r\n",
        "\r\n",
        "for layers in (base_model.layers)[:23]:\r\n",
        "    layers.trainable = False\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKCCtPJ-ipAD"
      },
      "source": [
        "\r\n",
        "# Add your desired classification head and initalize new model\r\n",
        "\r\n",
        "\r\n",
        "X= base_model.layers[-2].output\r\n",
        "predictions = Dense(3, activation=\"softmax\")(X)\r\n",
        "model = Model(base_model.input, predictions)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV73k0VDtZBA",
        "outputId": "93522111-1232-4f02-b4dc-28aa8cbf1f2a"
      },
      "source": [
        "# compile the model and print the summary\r\n",
        "model.compile(\r\n",
        "    loss=CategoricalCrossentropy(from_logits=False),\r\n",
        "    optimizer=Adam(learning_rate= learning_rate),\r\n",
        "    metrics=[\"accuracy\"],\r\n",
        ")\r\n",
        "model.summary()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 134,272,835\n",
            "Trainable params: 12,291\n",
            "Non-trainable params: 134,260,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEfCM7bqu05m",
        "outputId": "f2f32274-27bc-40e7-e60b-e43c5fc59740"
      },
      "source": [
        "\r\n",
        "# fit the model\r\n",
        "hist = model.fit(train_dataset, steps_per_epoch = math.ceil(train_examples/batch_size), epochs= epochs)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "48/88 [===============>..............] - ETA: 9:56 - loss: 1.3630 - accuracy: 0.3640 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-fe11a07f1d9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIAfQy4AvRVC"
      },
      "source": [
        "# plot the loss and accuracy curves\r\n",
        "\r\n",
        "loss = hist.history['loss']\r\n",
        "plt.plot(loss, label='Training Loss')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5eCwtfQc4fj"
      },
      "source": [
        "acc = hist.history['accuracy']\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxdjZRhPzrcZ"
      },
      "source": [
        "### Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb3YoB9_z_uV"
      },
      "source": [
        "In the feature extraction, you were only training a few layers on top of a base model. The weights of the pre-trained network were not updated during training.\r\n",
        "\r\n",
        "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9713lUTSzgAB"
      },
      "source": [
        "# unfreeze the base model\r\n",
        "# base_model.trainable = True\r\n",
        "\r\n",
        "\r\n",
        "for layers in (base_model.layers)[:23]:\r\n",
        "    layers.trainable = True\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1GD7SWk0QAt",
        "outputId": "4c409165-8f41-4ec1-99b5-4adb03e5cd40"
      },
      "source": [
        "# print the number of layers in the base model\r\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers in the base model:  23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmdw7PEE0rGE"
      },
      "source": [
        "You should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfhEc5-10cOw"
      },
      "source": [
        "# Decide the number of layers you want to freeze and freeze them.\r\n",
        "fine_tune_at = 21\r\n",
        "for layer in base_model.layers[:fine_tune_at]:  ## freezing layers\r\n",
        "  layer.trainable =  False\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt0CH-uH1GG6",
        "outputId": "45e1d5d1-e61d-4eba-dc8f-5c8fdda12ab1"
      },
      "source": [
        "# compile the model again.\r\n",
        "# It is advisable to use a lower learning rate here so that updates are not too huge and the model does not overfit.\r\n",
        "# print model summary\r\n",
        "\r\n",
        "model.compile(loss=CategoricalCrossentropy(from_logits=False),\r\n",
        "              optimizer = RMSprop(lr=learning_rate/10),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_41\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_64 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 3)                 12291     \n",
            "=================================================================\n",
            "Total params: 134,272,835\n",
            "Trainable params: 16,793,603\n",
            "Non-trainable params: 117,479,232\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "EhFgixwQ1nXA",
        "outputId": "7ea749a5-a7bb-4a58-e0c2-4ef19bc0f6b1"
      },
      "source": [
        "# fit the model again.\r\n",
        "fine_tune_epochs = 5\r\n",
        "total_epochs =  epochs + fine_tune_epochs\r\n",
        "\r\n",
        "history_fine = model.fit(train_dataset,\r\n",
        "                         epochs=total_epochs,\r\n",
        "                         steps_per_epoch = math.ceil(train_examples/batch_size),\r\n",
        "                         initial_epoch=hist.epoch[-1],\r\n",
        "                         )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 15/20\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 0.0492 - accuracy: 0.9890\n",
            "Epoch 16/20\n",
            "70/88 [======================>.......] - ETA: 2s - loss: 0.0398 - accuracy: 0.9914"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-415-930a7d76f7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                          \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                          \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                          )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "8Eh755Gk22ZX",
        "outputId": "43bbe26b-f868-499a-eaf9-def462a45e5c"
      },
      "source": [
        "# plot the loss and accuracy curves again\r\n",
        "loss = history_fine.history['loss']\r\n",
        "plt.plot(loss, label='Training Loss')\r\n",
        "plt.ylabel('Loss')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "acc = history_fine.history['accuracy']\r\n",
        "plt.plot(acc, label='Training Accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8NuaOTq3hLu"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntkRY1y_4f1g"
      },
      "source": [
        "Now we will use our model to make predictions on test set. Store the predictions like in sample_submission.csv file. \r\n",
        "\r\n",
        "sample_submission.csv\r\n",
        "- 4 columns\r\n",
        "- 1 row for each test image (total 853 rows)\r\n",
        "- 1st column = image file name\r\n",
        "- 2nd column = probability of the given image being qingqi\r\n",
        "- 3rd column = probability of the given image being rickshaw\r\n",
        "- 4th column = probability of the given image being tanga"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfqOBNCUWALd"
      },
      "source": [
        "def preprocessing_norm(images):\n",
        "    return preprocess_input(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YxNh0qo3Wi6"
      },
      "source": [
        "# Read and preprocess test data. Remeber to store the names of image files\r\n",
        "test_X = []\r\n",
        "img_file_names = []\r\n",
        "for file_path in glob.glob('/content/VehicleDataset/test/*'):\r\n",
        "  img = cv2.imread(file_path)\r\n",
        "  img.resize((input_shape))\r\n",
        "  img = preprocess_input(img)\r\n",
        "  test_X.append(img)\r\n",
        "  img_file_names.append(file_path.split('/')[-1].split('.')[0])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5nNCvDa4S8R"
      },
      "source": [
        "# predict on test data (Hint: model.predict())\r\n",
        "preds = model.predict(np.array(test_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87AHm6GQ5h6Q"
      },
      "source": [
        "# store the predictions in a .csv file like in sample_submission.csv (Hint: pandas may be helpful here)\r\n",
        "\r\n",
        "\r\n",
        "data = [['id', 'qingqi', 'rickshaw', 'tanga']]\r\n",
        "\r\n",
        "for index, pred in enumerate(preds):\r\n",
        "  file_name = img_file_names[index]\r\n",
        "  probabilites = list(pred)\r\n",
        "  probabilites.insert(0,file_name)\r\n",
        "  data.append(probabilites)\r\n",
        "\r\n",
        "data = np.array(data)\r\n",
        "df = pd.DataFrame(data=data[1:,0:],\r\n",
        "                  columns= data[0,0:])\r\n",
        "df.to_csv('result_new.csv', index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl7G2xypf3aG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixW4oaukf4FK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}