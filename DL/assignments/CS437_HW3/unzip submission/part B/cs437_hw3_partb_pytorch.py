# -*- coding: utf-8 -*-
"""CS437_HW3_PartB_PyTorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15k4T6iAUA73s5GtucRz-bMQZULkcUWdC

# Assignment 3  - Part B  - PyTorch - Secondary


#### Roll Number: 20030012

You have a choice in Part B. You can either choose to do using TensorFlow or using PyTorch. Both are equally good and are widely used in industry.

You are only required to do one of them. It's upto you to decide which one. However, you can also do both. In case you do both, you have to choose one as primary and and other one as secondary. Your primary one will be marked as normal and secondary one will marked for a 10% bonus.

For example, if you do both and choose tensorflow as your primary attempt then you should primary in first heading of tensorflow notebook (Assignment 3  - Part B  - TensorFlow - Primary) and secondary (Assignment 3  - Part B  - PyTorch - Secondary) in pytorch notebook.

In case, you only do one then you just need to submit that notebook.

### Task Explanation

In this part we will implement, train and evaluate a neural network using tensorflow on wheat disease classification problem. 

Wheat rust is a devastating plant disease that affects many crops, reducing yields and affecting the livelihoods of farmers and decreasing food security across the continent. The disease is difficult to monitor at a large scale, making it difficult to control and eradicate.

The objective of this challenge is to build a machine learning algorithm to correctly classify if a plant is healthy, has stem rust, or has leaf rust.

### Let's Start

Make necessary imports here e.g. import cv2, import glob, etc
"""

import matplotlib.pyplot as plt
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchsummary import summary
import glob
from sklearn.metrics import confusion_matrix
import pandas as pd
import seaborn as sns
# any other imports that you may require
import random
from sklearn.metrics import confusion_matrix
import math
import cv2
import glob

"""The following code decides whether to run on GPU or CPU. You can have a look [here](https://pytorch.org/docs/stable/notes/cuda.html) to understand how to use this here in this task."""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""### Hyperparameters

You can play with these to improve accuracy on test data.
"""

batch_size = 32
epochs = 10
learning_rate = 0.001
input_shape = (3,256,256)

"""### Data

Get Wheat Disease Data for training and testing
"""

!git clone https://github.com/MMFa666/WheatDiseaseDataset.git

"""Get paths for all the training images in the dataset and print the length of training_paths list. For this purpose you can use glob. You can have a look [here](https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/) on how to use glob."""

# code here
train_paths = []
for type_folder in glob.glob('/content/WheatDiseaseDataset/train/*'):
  for img_link in glob.glob(type_folder + '/*'):
    train_paths.append(img_link)

train_examples = len(train_paths)

"""Do the same for testing data images."""

# code here
test_paths = []
for type_folder in glob.glob('/content/WheatDiseaseDataset/test/*'):
  for img_link in glob.glob(type_folder + '/*'):
    test_paths.append(img_link)
test_examples = len(test_paths)

"""### Labels"""

labels={}
labels['healthy_wheat'] = 0
labels['leaf_rust'] = 1
labels['stem_rust'] = 2

"""### Preprocessing

Preprocessing of data such as normalization, mean shift, make the learning task simple for network and could accelerate the training process. In this task, we will only do normaliztion.

In images, pixel values range from 0 to 255. To shift the values between (0,1) range, divide input image by 255.
"""

def preprocessing_norm(images):
    return images/255

"""### Batch Generator

Previously, when training our models, we were loading the complete data in memory to fit our model. However, in practice we're working with very large datasets which cannot be loaded all at once in memory. As a solution, we use "Data Generators" which are essentially python generators that load batches of data from disk into memory and pass into our models. In order to achieve this, we only store filepaths that point to training/test samples in our dataset in memory. A data generator yields a tuple of (Xs,Ys) whenever the generator is used via the next() function. 

For examples of batch_generators, you can have a look [here](https://www.geeksforgeeks.org/generators-in-python/) or [here](https://www.programcreek.com/python/?CodeExample=generate+batches). Essentially you have to the following:
- Shuffle the paths to get a uniform distribution in all batches.
- Divide paths into batches.
- Read image from the path. (Remeber cv2 reads image in BGR format.)
- Resize each image to input_shape.
- Pytorch accepts channels-first input style. So transpose the images. (Hint: see np.transpose())
- Extract label of the image from the image path using folder name. (Hint: You can do this by splitting the path.)
- No need to one-hot encode here.
- Yield images and labels in tuple.
"""

# Batch generator function here.
def data_generator(file_path):

  batch_train_paths = [file_path[i:i+batch_size] for i in range(0,len(file_path),batch_size)] ## 21 batches 
  for batch in np.tile(batch_train_paths,epochs): ## generating/yielding batches by #of epochs x epoch step size = 30 * 21 = 630
    one_hot_labels = []
    imgs_list = []
    for path in batch:
      img = preprocessing_norm(cv2.imread(path))
      img.resize((input_shape)) ## (256,256,3)
      imgs_list.append(img)
      imgs_label = path.split('/')[4] ## getting label name
      one_hot_encoded_label = np.zeros(3)
      one_hot_encoded_label[labels[imgs_label]] = 1
      one_hot_labels.append(one_hot_encoded_label)
    yield np.array(imgs_list), np.array(one_hot_labels)

"""Initialize train data generator"""

# code here
random.shuffle(train_paths)
generator = data_generator(train_paths)

"""### Training

Define network class here. You can have a look [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network) to understand how this works.

Model Architecture:
- See model.png for model architecture
- Filter size in each convolution layer is 3 except first convolution where it is 5.
- The stride in each convolution layer is 2.
- There is no padding in convolution layer.
- Dropout = 0.2 on each dropout layer.
- The last layer has no activation. The loss function will add softmax activation by itself.
- The model should have 490,771 total parameters.
"""

model = nn.Sequential(
    
    nn.Conv2d(3,256, 5, 2),
    nn.BatchNorm2d(256),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Conv2d(256,128,3,2),
    nn.BatchNorm2d(128),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Conv2d(128,64,3,2),
    nn.BatchNorm2d(64),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Conv2d(64,32,3,2),
    nn.BatchNorm2d(32),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Conv2d(32,16,3,2),
    nn.BatchNorm2d(16),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Flatten(),
    nn.Linear(576,128),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Linear(128,32),
    nn.LeakyReLU(0.1),
    nn.Dropout(0.2),

    nn.Linear(32,3)
)

model

"""Initialize network and cast it to device declared above. (Hint: net.to(device) )"""

# code here
model.to(device)

#print model summary here
summary(model,(3,256,256))

"""Define [CrossEntropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) and [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer here. (Hint: See the links to understand how to do this.)"""

# Code here
cross_entropy_loss = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), learning_rate)

# Intialize num_training_batches.
num_training_batches=len(train_paths)//batch_size

"""Train the network here. You can have look [here](num_training_batches) to understand how to do this. Put the network to training mode before training so that dropout layers work as they are supposed to. (Hint: your_model.train().) Remember to convert your data and labels to pytorch tensors and cast to device before passing it to the network. Also, record the training loss for plotting purposes."""

# code here
model.training = True

generator = data_generator(train_paths)
train_loss_hist = []

for epoch in range(epochs):  # loop over the dataset multiple times
    print('\n\nepoch: ', epoch)
    running_loss = 0.0
    for i in range(num_training_batches):

        x_train,data_labels =next(iter(generator))

        x_train = torch.from_numpy(x_train).to(device)
        data_labels = torch.from_numpy(data_labels).to(device)
        data_labels = np.argmax(data_labels, axis = 1) ## reverseing from one hot encoding

        optimizer.zero_grad()

        outputs = model(x_train.float())
        loss = cross_entropy_loss(outputs, data_labels)
        print('\nloss in  training batch : ', i+1, ' is : ', loss)
        train_loss_hist.append(loss)

        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

"""Plot the loss graph of training."""

# Loss Plot
plt.plot(train_loss_hist)
plt.show()

"""### Evaluation

Now, we will evaluate our model on the test data.

First, let's read the test data using test_paths. Similar to what we did in batch_generator.
"""

# random.shuffle(test_paths)
test_generator = data_generator(test_paths)

"""Put the network in eval mode using your_model.eval()"""

# code here
model.eval()

predictions = []
actual_predictions = []
num_test_batches = len(test_paths)//batch_size
for i in range(num_test_batches):  ##5 
  # print('test sample : ', i+1)
  x_test, y_test =next(iter(test_generator))

  x_test = torch.from_numpy(x_test).to(device)
  y_test = torch.from_numpy(y_test).to(device)
  y_test = np.argmax(y_test, axis = 1) ## reverseing from one hot encoding
  actual_predictions.append(y_test)

  pred_y = model(x_test.float())
  predictions.append(pred_y)

"""Now, make predictions on test data. Cast the outputs to cpu and convert to numpy (using outputs.cpu().numpy()). If you get memory error then you can loop over all images one by one and then make and store the prediction.

Extract class label from predictions. (Hint: you can use np.argmax() ).
"""

# code here
for i in range(num_test_batches):
  predictions[i] = np.argmax(predictions[i].detach().numpy(), axis=1)

"""Calculate and print accuracy."""

# actual_predictions[0] == predictions[0]
correct = 0
for batch_num in range(len(actual_predictions)):
  for preds in range(len(predictions)):
    if(predictions[batch_num][preds] == actual_predictions[batch_num][preds]):
      correct+=1
print('acc : ', (correct/test_examples)*100)

actual_predictions1 = np.array([i.detach().numpy() for i in actual_predictions]).flatten()
predictions1= np.array(predictions).flatten()

"""Calculate and print Confusion Matrix. Have a look [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) for this."""

# code here
conf_mat = confusion_matrix(actual_predictions1, predictions1)
print(conf_mat)

def plot_confusion_matrix(conf_mat):
    classes = list(labels.keys())
    df_cm = pd.DataFrame(conf_mat,classes,classes)
    plt.figure(figsize=(10,7))
    sns.set(font_scale=1.4)
    sns.heatmap(df_cm, annot=True,annot_kws={"size": 16})
    plt.show()

"""Use the above function to plot confusion matrix here."""

# code below
plot_confusion_matrix(conf_mat)

"""### Questions

What is overfitting? How are we trying to prevent overfitting here?

Answer:

What is class imbalance? How does it effect training? Does this training set have class imbalance? If yes, then show it (using numbers).

Answer:
"""

