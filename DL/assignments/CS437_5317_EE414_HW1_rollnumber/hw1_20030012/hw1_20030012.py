# -*- coding: utf-8 -*-
"""hw1_20030012.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PaXo-2dKJrQu0Ap4NW2z8kZrYEZAZV6N

**Colab Notebook Sharing Link(edit access)**:
https://colab.research.google.com/drive/1PaXo-2dKJrQu0Ap4NW2z8kZrYEZAZV6N?usp=sharing

# CS 437 - Deep Learning - PA1:Neural Network Class

*__Submission Instructions:__*
- Rename this notebook to `hw1_rollnumber.ipynb` before submission on LMS.
- All code must be written in this notebook (you do not need to submit any other files).
- The output of all cells must be present in the version of the notebook you submit. You will be penalized if the output is absent.
- The university honor code should be maintained. Any violation, if found, will result in disciplinary action. 
- You have to download assignment from LMS and do all your workings on colab. Don't download the files again and again.
- You can share the notebook link of your colab noteboob in the begining of the notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import time
# %matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn')
from IPython.display import Image
import pydot
from tqdm import tqdm_notebook
import seaborn as sns
from keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split
from keras.utils import np_utils
from sklearn.datasets import make_moons
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import confusion_matrix,classification_report

"""**Please write your roll number in the next cell**"""

rollnumber = 20030012

"""In this assignment you will be creating 4 versions of the `NeuralNetwork` class. You will start with a simple 2 layer feed forward network and progressively modify the class by adding features to make it more generic. At the end you will have implemented a version which can create networks of arbitrary shape and depth, and which will work for both regression and classification tasks. 

Often in pratical situations, raw machine learning architecture and code is hidden behind libraries and simplfied toolkits. The purpose of this assignment is to lift that curtain and get you hands-on exprience working with the mathematical fundamentals of neural network architectures. After this, you'll know exactly how a network leverages 'gradient descent' to find optimal solutions and how forward and backward passes are implemented mathematically and in code.


### Primary Task
Skeleton code is provided to get you started; the main methods you need to implement correspond to the 3 steps of the training process, namely:
1. Initialize variables and initialize weights
2. Forward pass
3. Backward pass AKA Backpropogation
4. Weight Update AKA Gradient Descent

__Look for comments in the code to see where you are supposed to write code.__ Essentially, you will be working on 5 functions. 
You should use the lecture [slides]() as reference for the equations. 

A `fit` function is what combines the previous three functions and overall trains the network to __fit__ to the provided training examples. In all the following tasks, the provided `fit` methods require the three steps of the training process to be correctly working. The function has been setup in a way that it expects the above 3 methods to take particular inputs and return particular outputs. __You are supposed to work within this restriction.__ Modification of the provided code without prior discussion with the TAs will result in a __grade deduction__. 



To see how well your model is doing, you need to look at the dummy tasks (at the end) and make sure your model loss is going down during training. A dummy regression task of adding two numbers (sum less than 1) has been provided as well. Similarly, a dummy classification task (XOR logic gate) is also present. You can look at the shapes of the inputs and outputs matrices as well as the training trend (once you implement a full task) by using your own class (make sure you are using the correct arguments to the `__init__` method). 

You can find a demonstration of the neural network working on a synthetic dataset for both regression and classification at the end of the notebook. After you implement your class fully, you can play with the parameters and see the visualization change, we highly recommend that you try this. This part of the notebook will not be graded in any way, but it might give you a better insight/intuition into how the model makes decisions, and how important parameters are in terms of the usefullness of neural networks. You will explore the parameter space more thoroughly in the next assignment :P

### Side note
*The `plot_model` method will only work if you have the `pydot` python package installed along with [Graphviz](https://graphviz.gitlab.io/download/)). If you do not wish to use this then simply comment out the import for `pydot`.*

### Need Help?
If you need help, refer to your textbook (provided on LMS) which has examples and explanations for all the processes you'll have to implement, as well as rich details on functions such as `sigmoid` and `softmax`. Going over the book once before getting started is a good idea, you can also refer to the class slides and supplemental material provided with the assignment.

## Task 1

In this task you will implement the simplest version of a feed forward neural network - a 2 layer network. 

Your code will only be partially vectorized, this means that you will be passing a single data point through the network at a time. In simple terms, the running time of your `fit` method will be $O(e*n)$ where $e$ is the number of epochs and $n$ is the number of data points (assuming all functions/methods called in `fit` take constant time). 

This version of the network will be using the `softmax` activation function for the output layer and `sigmoid` for the hidden layer, *ie.* a classification model which learns to output the joint probability mass function of the classes in the dataset.
"""

def plot_confusion_matrix(conf_mat):
    classes = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']
    df_cm = pd.DataFrame(conf_mat,classes,classes)
    plt.figure(figsize=(15,9))
    sns.set(font_scale=1.4)
    sns.heatmap(df_cm, annot=True,annot_kws={"size": 16})
    plt.show()

class_labels = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']

class NeuralNetwork():
    @staticmethod
    def cross_entropy_loss(y_pred, y_true):  ## y_true = 60000 x 10 , y_pred = 60000x10
        # implement cross_entropy_loss function
        #TO DOmake 2D array stack vertically python

        loss = -1 * np.sum(np.multiply(np.log(y_pred+ 1e-8), y_true), axis=1) ## 60000 x 1
        return np.sum(loss) ### adding losses for all examples

    @staticmethod
    def accuracy(y_pred, y_true):
        # implement accuracy function
        #TO DO
        correct = 0
        for i in range(len(y_pred)):
          if y_pred[i] == y_pred[i]:
            correct = correct + 1
        
        return (correct / len(y_pred)) *100


    @staticmethod
    def softmax(x):

        # implement softmax function
        #TO DO
      return np.exp(x) / np.sum(np.exp(x))

    @staticmethod
    def sigmoid(x):
        #TO DO
        return 1 / (1 + np.exp(-x))

    
    def __init__(self, input_size, hidden_nodes, output_size):
        '''Creates a Feed-Forward Neural Network.
        The parameters represent the number of nodes in each layer (total 3). 
        Look at the inputs to the function'''
        self.num_layers = 3  # includes input layer
        self.input_shape = input_size
        self.hidden_shape = hidden_nodes
        self.output_shape = output_size
        self.weights_ = []
        self.biases_ = []
        self.__init_weights()

    def __init_weights(self):
        '''Initializes all weights based on standard normal distribution and all biases to 0.'''
        W_h = np.random.normal(size=(784, 80))
        b_h = np.zeros(shape=(1,))
        
        W_o = np.random.normal(size=(80, 10))
        b_o = np.zeros(shape=(1,))
        
  
        self.weights_.append(W_h)
        self.weights_.append(W_o)

        self.biases_.append(b_h)
        self.biases_.append(b_o)

    
    def forward_pass(self, input_data):
        '''Executes the feed forward algorithm.
        "input_data" is the input to the network in row-major form
        Returns "activations", which is a list of all layer outputs (excluding input layer of course)'''

        dot_product_h = []
        dot_product_o= []
        activations = []
        input_data = input_data.flatten()
        for i in range(self.weights_[0].shape[1]): ## 80
          dot = []
          for j in range(len(input_data)): ## 784
            dot.append(input_data[j] * self.weights_[0][j][i])
          dot_product_h.append(np.sum(dot) + self.biases_[0])
        dot_product_h = np.array(dot_product_h).reshape((1,80))
        sigmoid_h = self.sigmoid(dot_product_h)  ## 1x80
        activations.append(sigmoid_h)

        for i in range(self.weights_[1].shape[1]): ## 10
          dot = []
          for j in range(len(sigmoid_h)): ## 80
            dot.append(sigmoid_h[j] * self.weights_[1][j][i])
          dot_product_o.append(np.sum(dot) + self.biases_[1])
        dot_product_o = np.array(dot_product_o).reshape((1,10))
        softmax_o = self.softmax(dot_product_o)  ## 1x10
        activations.append(softmax_o)

        return activations
    
    def backward_pass(self, targets, layer_activations, input_data):
        '''Executes the backpropogation algorithm.
        "targets" is the ground truth/labels
        "layer_activations" are the return value of the forward pass step
        Returns "deltas", which is a list containing weight update values for all layers (excluding the input layer of course)'''        
        
        deltas = []
        ## for output layer    

        delta_y = -1* (targets - np.array(layer_activations[-1]))  ## -(t - y)    1x10 
        delta_y = delta_y.flatten()
        delta_w2 = delta_y  ## delta_w2 = dE/dy * dy/dz = (pred - actual)  ## 1x10  ## softmax derivative with cross entropy
        deltas.append(delta_w2)

        ## for hidden layer##########################

        delta_y_h = []
        delta_z_h = []
        delta_w1 = []
        delta_w2_trans = np.transpose(delta_w2)  ## 10x1
        for i in range(self.weights_[1].shape[0]):  ## 80
          dot = []
          for j in range(delta_w2_trans.shape[0]): ## 10
            dot.append(self.weights_[1][i][j] *  delta_w2_trans[j])
          delta_y_h.append(np.sum(dot))
        delta_y_h = np.array(delta_y_h).reshape((1,80))
        delta_z_h = layer_activations[0]*(1- layer_activations[0])  ## yh1(1-yh1)  sigmoid delta 1x80
        delta_z_h = delta_z_h.flatten().reshape((1,80))
        for i in range(len(delta_z_h)):
          delta_w1.append(delta_z_h[i] * delta_y_h[i])
        np.array(delta_w1).reshape((1,80))
        deltas.insert(0, delta_w1)

        return deltas 

    def weight_update(self, deltas, layer_inputs, lr):
        '''Executes the gradient descent algorithm.
        "deltas" is return value of the backward pass step
        "layer_inputs" is a list containing the inputs for all layers (including the input layer)
        "lr" is the learning rate'''
        layer_inputs[0] = layer_inputs[0].reshape(1,784)
        layer_inputs[-1] = layer_inputs[-1].reshape(1,80)
        delta_0_trans = np.transpose(deltas[0]).reshape((80,1))
        delta_1_trans = np.transpose(deltas[-1]).reshape((10,1))

        grad1 = []
        grad2 = []
        for i in range(delta_0_trans.shape[0]): ## 80 
          dot =[]
          for j in range(layer_inputs[0].shape[1]): ## 784
            dot.append(delta_0_trans[i][0] * layer_inputs[0][0][j])
          grad1.append(dot)
        grad1 = np.array(grad1).T.reshape((784, 80))   

        for i in range(delta_1_trans.shape[0]): ## 10
          dot =[]
          for j in range(layer_inputs[-1].shape[1]): ## 80
            dot.append(delta_1_trans[i][0] * layer_inputs[-1][0][j])
          grad2.append(dot)
        grad2 = np.array(grad2).T.reshape((80,10))

        self.weights_[0] = self.weights_[0] - (lr*grad1)
        self.weights_[1] = self.weights_[1] - (lr*grad2)
        self.biases_[1] = self.biases_[1] - (lr*np.sum(deltas[-1]))
        self.biases_[0] = self.biases_[0] - (lr*np.sum(deltas[0]))


    
    def fit(self, Xs, Ys, epochs, lr=1e-3):
            history = []
            # for 
            for epoch in tqdm_notebook(range(epochs)):
                print('\n\n\n\nfor epoch : ', epoch)
                num_samples = Xs.shape[0]
                for i in range(num_samples):
                    sample_input = Xs[i,:].reshape((1,self.input_shape))   ## 1 x 784 
                    sample_target = Ys[i,:].reshape((1,self.output_shape))   ## 1 x 10
                    activations = self.forward_pass(sample_input) # Call forward_pass function 
                    deltas = self.backward_pass(sample_target, activations, sample_input.flatten()) # Call backward_pass function 
                    layer_inputs = [sample_input] + activations[:-1]
                    self.weight_update(deltas, layer_inputs, lr)


                preds = self.predict(Xs) # Call predict function 
                current_loss = self.cross_entropy_loss(preds, Ys)
                # print('loss: ', current_loss)
                if  epoch==epochs-1:
                  # confusion_mat=confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1))  
                  confusion_mat = confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1),labels=np.arange(10))
                  plot_confusion_matrix(confusion_mat)
                  # report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1)), target_names=class_labels)
                  report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1),num_classes=classes), target_names=class_labels)
                  print(report)
                history.append(current_loss)
            return history
    
    def predict(self, Xs):
        '''Returns the model predictions (output of the last layer) for the given "Xs".'''
        predictions = []
        num_samples = Xs.shape[0]
        for i in range(num_samples):
            sample = Xs[i,:].reshape((1,self.input_shape))
            sample_prediction = self.forward_pass(sample)[-1]
            predictions.append(sample_prediction.reshape((self.output_shape,)))  ## 10, 
        return np.array(predictions)
    
    def evaluate(self, Xs, Ys):
        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''
        pred = self.predict(Xs)
        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))
    
    def plot_model(self, filename):
        '''Provide the "filename" as a string including file extension. Creates an image showing the model as a graph.'''
        graph = pydot.Dot(graph_type='digraph')
        graph.set_rankdir('LR')
        graph.set_node_defaults(shape='circle', fontsize=0)
        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]
        for i in range(self.num_layers-1):
            for n1 in range(nodes_per_layer[i]):
                for n2 in range(nodes_per_layer[i+1]):
                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')
                    graph.add_edge(edge)
        graph.write_png(filename)

classes = 10
#Download Fashion MNIST dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
#Split the fashion MNIST dataset into train, validation and test sets (Done for you)
#Convert y_train,y_val and y_test to categorical binary values 
#TO DO

y_train = y_train.reshape(y_train.shape[0],1)
y_test = y_test.reshape(y_test.shape[0], 1)
y_train = np_utils.to_categorical(y_train, num_classes = classes)
y_test = np_utils.to_categorical(y_test, num_classes = classes)


#See function "np_utils.to_categorical()"
#Reshape images of X_train and X_test to 1d array


X_train = np.array([image.reshape(1,784) for image in X_train])
X_test = np.array([image.reshape(1,784) for image in X_test])

#TO DO

#See function of numpy reshape

X_train = X_train[0:5000,:]
y_train = y_train[0:5000,:]


# starting time
start = time.time()
# Define the input size and output size of  Fashion MNIST dataset



nn = NeuralNetwork(input_size=784, hidden_nodes=80, output_size=10)
# You can tweak the learning rate and epochs to know how things work
history = nn.fit(X_train, y_train, epochs=10, lr=0.0005)

plt.plot(history);

plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));
# end time
end = time.time()
print("Runtime of the algorithm is ",round((end - start),3)," seconds")

#

"""## Task 2

Now you will modify the class to allow the option to learn a regression model. You need to change some methods to account for the `mode` of the network. You can copy your code from task 1 as a starting point if you want.

If the `mode` is classification, you will use your code from Task1. In case of the `mode` being regression you will apply the `sigmoid` activation function to both layers and compute the deltas for that.
"""

class NeuralNetwork():
    @staticmethod
    def mean_squared_error(y_pred, y_true):
        # implement mean_squared_error function
        #TO DO
        return (1/len(y_true)) * (np.sum((y_true - y_pred) ** 2))

    @staticmethod
    def cross_entropy_loss(y_pred, y_true):
        # implement cross_entropy_loss function
        #TO DO
        loss = -1 * np.sum(np.multiply(np.log(y_pred+ 1e-8), y_true), axis=1)
        return np.sum(loss) ### adding losses for all examples

    @staticmethod
    def accuracy(y_pred, y_true):
        # implement accuracy function
        correct = 0
        for i in range(len(y_pred)):
          if y_pred[i] == y_pred[i]:
            correct = correct + 1
        return (correct / len(y_pred)) *100

    @staticmethod
    def softmax(x):
        # implement softmax function
        #TO DO
        return np.exp(x) / np.sum(np.exp(x))

    @staticmethod
    def sigmoid(x):
        #TO DO
        return 1 / (1 + np.exp(-x))
    
    
    def __init__(self, input_size, hidden_nodes, output_size, mode):
        '''Creates a Feed-Forward Neural Network.
        "mode" can be one of 'regression' or 'classification' and controls the output activation as well as training metric
        The rest of the parameters represent the number of nodes in each layer (total 3).'''
        if mode not in ['classification','regression']:
            raise ValueError('Only "classification" and "regression" modes are supported.')
        
        self.num_layers = 3 # includes input layer
        self.input_shape = input_size
        self.hidden_shape = hidden_nodes
        self.output_shape = output_size
        self.mode = mode
        
        
        self.weights_ = []
        self.biases_ = []
        self.__init_weights()
    
    def __init_weights(self):
        '''Initializes all weights based on standard normal distribution and all biases to 0.'''
        W_h = np.random.normal(size=(self.input_shape, self.hidden_shape))
        b_h = np.zeros(shape=(1,))
        
        W_o = np.random.normal(size=(self.hidden_shape, self.output_shape))
        b_o = np.zeros(shape=(1,))
        
        self.weights_.append(W_h)
        self.weights_.append(W_o)

        self.biases_.append(b_h)
        self.biases_.append(b_o)
    
    
    def forward_pass(self, input_data):
        '''Executes the feed forward algorithm.
        "input_data" is the input to the network in row-major form
        Returns "activations", which is a list of all layer outputs (excluding input layer of course)'''
        
        dot_product_h = []
        dot_product_o= []
        activations = []
        for i in range(self.weights_[0].shape[1]): ## 6
          dot = []
          for j in range(len(input_data)): ## 2
            dot.append(input_data[j] * self.weights_[0][j][i]) ##### 1x2   2x6  = 1x6
          dot_product_h.append(np.sum(dot) + self.biases_[0])
        dot_product_h = np.array(dot_product_h).reshape((1,self.hidden_shape))
        sigmoid_h = self.sigmoid(dot_product_h)  ## 1x6
        activations.append(sigmoid_h)

        for i in range(self.weights_[1].shape[1]): ## 1
          dot = []
          for j in range(len(sigmoid_h)): ## 6
            dot.append(sigmoid_h[j] * self.weights_[1][j][i])             ## 1x6  6 x1
          dot_product_o.append(np.sum(dot) + self.biases_[1])
        dot_product_o = np.array(dot_product_o).reshape((1,self.output_shape))
        output_o = self.softmax(dot_product_o) if self.mode == 'classification' else self.sigmoid(dot_product_o)
        activations.append(output_o)

        return activations
    
    def backward_pass(self, targets, layer_activations):
        '''Executes the backpropogation algorithm.
        "targets" is the ground truth/labels
        "layer_activations" are the return value of the forward pass step
        Returns "deltas", which is a list containing weight update values for all layers (excluding the input layer of course)'''

        deltas = []
        delta_w2 = []
        if self.mode == 'classification':
          delta_y = -1* (targets - np.array(layer_activations[-1]))  ## -(t - y)    1x10 .. derivative of cross entropy loss
          delta_w2 = delta_y.flatten()
        else:
          delta_y = (-1* (targets - np.array(layer_activations[-1]))).flatten()  ## -(t - y)    1x10 
          delta_z = (layer_activations[-1]*(1- layer_activations[-1])).flatten()  ## y01(1-y01)  sigmoid delta
          for i in range(len(delta_y)):
            delta_w2.append(delta_z[i] * delta_y[i])
        deltas.append(delta_w2)
        # for hidden layer##########################
        delta_y_h = []
        delta_z_h = []
        delta_w1 = []
        delta_w2_trans = np.transpose(delta_w2)  
        for i in range(self.weights_[1].shape[0]):  
          dot = []
          for j in range(delta_w2_trans.shape[0]):
            dot.append(self.weights_[1][i][j] *  delta_w2_trans[j])
          delta_y_h.append(np.sum(dot))
        delta_y_h = np.array(delta_y_h).reshape((1,self.hidden_shape))
        delta_z_h = layer_activations[0]*(1- layer_activations[0])  ## yh1(1-yh1)  sigmoid delta 
        delta_z_h = delta_z_h.flatten().reshape((1,self.hidden_shape))
        for i in range(len(delta_z_h)):
          delta_w1.append(delta_z_h[i] * delta_y_h[i])
        np.array(delta_w1).reshape((1,self.hidden_shape))
        deltas.insert(0, delta_w1)
        
        return deltas
    
    def weight_update(self, deltas, layer_inputs, lr):
        '''Executes the gradient descent algorithm.
        "deltas" is return value of the backward pass step
        "layer_inputs" is a list containing the inputs for all layers (including the input layer)
        "lr" is the learning rate'''

        layer_inputs[0] = layer_inputs[0].reshape(1,self.input_shape)
        layer_inputs[-1] = layer_inputs[-1].reshape(1,self.hidden_shape)
        delta_0_trans = np.transpose(deltas[0]).reshape((self.hidden_shape,1))
        delta_1_trans = np.transpose(deltas[-1]).reshape((self.output_shape,1))

        grad1 = []
        grad2 = []
        for i in range(delta_0_trans.shape[0]):
          dot =[]
          for j in range(layer_inputs[0].shape[1]):
            dot.append(delta_0_trans[i][0] * layer_inputs[0][0][j])
          grad1.append(dot)
        grad1 = np.array(grad1).T.reshape((self.input_shape, self.hidden_shape))   

        for i in range(delta_1_trans.shape[0]):
          dot =[]
          for j in range(layer_inputs[-1].shape[1]):
            dot.append(delta_1_trans[i][0] * layer_inputs[-1][0][j])
          grad2.append(dot)
        grad2 = np.array(grad2).T.reshape((self.hidden_shape,self.output_shape))

        self.weights_[0] = self.weights_[0] - (lr*grad1)
        self.weights_[1] = self.weights_[1] - (lr*grad2)
        self.biases_[1] = self.biases_[1] - (lr*np.sum(deltas[-1]))
        self.biases_[0] = self.biases_[0] - (lr*np.sum(deltas[0]))

        
    def fit(self, Xs, Ys, epochs, lr=1e-3):
        '''Trains the model on the given dataset for "epoch" number of itterations with step size="lr". 
        Returns list containing loss for each epoch.'''
        history = []
        for epoch in tqdm_notebook(range(epochs)):
            print('for epoch: ', epoch)
            num_samples = Xs.shape[0]
            for i in range(num_samples):
                if(i+2 <= num_samples):
                    sample_input = Xs[i:i+2,:].reshape((1,self.input_shape))
                    sample_target = Ys[i,:].reshape((1,self.output_shape))
                    
                    activations = self.forward_pass(sample_input) # Call forward_pass function 
                    deltas = self.backward_pass(sample_target, activations) # Call backward_pass function 
              
                    layer_inputs = [sample_input] + activations[:-1]
                    # Call weight_update function
                    self.weight_update(deltas, layer_inputs, lr)

            preds = self.predict(Xs) # Call predict function 
            if self.mode == 'regression':
                current_loss = self.mean_squared_error(preds, Ys[:-1])
            elif self.mode == 'classification':
                current_loss = self.cross_entropy_loss(preds, Ys[:-1])
            history.append(current_loss)
        return history
    
    def predict(self, Xs):
        '''Returns the model predictions (output of the last layer) for the given "Xs".'''
        predictions = []
        num_samples = Xs.shape[0]
        for i in range(num_samples):
            if(i+2 <= num_samples):
                sample = Xs[i:i+2,:].reshape((1,self.input_shape))
                sample_prediction = self.forward_pass(sample)[-1]
                predictions.append(sample_prediction.reshape((self.output_shape,)))
        return np.array(predictions)
    
    def evaluate(self, Xs, Ys):
        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''
        pred = self.predict(Xs)
        if self.mode == 'regression':
            return self.mean_squared_error(pred, Ys)
        elif self.mode == 'classification':
            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))
    
    def plot_model(self, filename):
        '''Provide the "filename" as a string including file extension. Creates an image showing the model as a graph.'''
        graph = pydot.Dot(graph_type='digraph')
        graph.set_rankdir('LR')
        graph.set_node_defaults(shape='circle', fontsize=0)
        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]
        for i in range(self.num_layers-1):
            for n1 in range(nodes_per_layer[i]):
                for n2 in range(nodes_per_layer[i+1]):
                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')
                    graph.add_edge(edge)
        graph.write_png(filename)

nn = NeuralNetwork(input_size=2, hidden_nodes=5, output_size=1, mode='regression')
nn.plot_model('graph.png')
Image('graph.png')

data_x, _ = make_moons(200, noise=0.18)
plt.scatter(data_x[:,0], data_x[:,1]);

reg = LinearRegression()
reg = reg.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));

d = np.arange(-1.5, 2.5, 0.1).reshape((40,1))
preds = reg.predict(d)

fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Linear Regression {}'.format(rollnumber));
ax.legend();

# starting time
start = time.time()

nn = NeuralNetwork(input_size=2, hidden_nodes=6, output_size=1, mode='regression')
history = nn.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)), epochs=40, lr=0.005)
preds = nn.predict(d)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));
# end time
end = time.time()
print("Runtime of the algorithm is ",round((end - start),3)," seconds")

"""## Task 3

Again, if it helps copy only your code from Task2 as a starting point in the next cell. Now you will modify the class to allow an arbitrarily shaped network. 

*Hint: The output/last layer is special in terms of the delta calculation. All the hidden layers have the same calculation (chain rule)*
"""

class NeuralNetwork():
    @staticmethod
    def mean_squared_error(y_pred, y_true):
        # implement mean_squared_error function
        #TO DO
        return (1/len(y_true)) * (np.sum((y_true - y_pred) ** 2))

    @staticmethod
    def cross_entropy_loss(y_pred, y_true):
        # implement cross_entropy_loss function
        #TO DO

        loss = -1 * np.sum(np.multiply(np.log(y_pred), y_true), axis=1)
        return np.sum(loss) ### adding losses for all examples
    
    @staticmethod
    def accuracy(y_pred, y_true):
        # implement accuracy function
        correct = 0
        for i in range(len(y_pred)):
          if y_pred[i] == y_pred[i]:
            correct = correct + 1
        return (correct / len(y_pred)) *100

    @staticmethod
    def softmax(x):
        # implement softmax function
        #TO DO
        return np.exp(x) / np.sum(np.exp(x))

    @staticmethod
    def sigmoid(x):
        #TO DO
        return 1 / (1 + np.exp(-x))
        
    
    def __init__(self, nodes_per_layer, mode):
        '''Creates a Feed-Forward Neural Network.
        "nodes_per_layer" is a list containing number of nodes in each layer (including input layer)
        "mode" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''
        if len(nodes_per_layer) < 2:
            raise ValueError('Network must have atleast 2 layers (input and output).')
        if not (np.array(nodes_per_layer) > 0).all():
            raise ValueError('Number of nodes in all layers must be positive.')
        if mode not in ['classification','regression']:
            raise ValueError('Only "classification" and "regression" modes are supported.')
        
        self.num_layers = len(nodes_per_layer) # includes input layer
        self.nodes_per_layer = nodes_per_layer
        self.input_shape = nodes_per_layer[0]
        self.output_shape = nodes_per_layer[-1]
        self.mode = mode
        
        
        self.__init_weights(nodes_per_layer)
    
    def __init_weights(self, nodes_per_layer):
        '''Initializes all weights based on standard normal distribution and all biases to 0.'''
        self.weights_ = []
        self.biases_ = []
        for i,_ in enumerate(nodes_per_layer):
            if i == 0:
                # skip input layer, it does not have weights/bias
                continue
            
            weight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))
            self.weights_.append(weight_matrix)
            bias_vector = np.zeros(shape=(1,))
            self.biases_.append(bias_vector)
    

    def forward_pass(self, input_data):
        '''Executes the feed forward algorithm.
        "input_data" is the input to the network in row-major form
        Returns "activations", which is a list of all layer outputs (excluding input layer of course)'''
        input_data = input_data.flatten() ## 1x2
        activations = []

        for layer_num,nodes in enumerate(self.nodes_per_layer):
          if layer_num == 0:
                # skip input layer, it does not have weights/bias
              continue
        
          dot_product_h = []
          for i in range(self.weights_[layer_num-1].shape[1]): ## 5
            dot = []
            for j in range(len(input_data)): ## 2
              dot.append(input_data[j] * self.weights_[layer_num-1][j][i]) ##### 1x2   2x5  = 1x5
            dot_product_h.append(np.sum(dot) + self.biases_[layer_num-1])
          dot_product_h = np.array(dot_product_h).reshape((1, nodes))
          if self.mode == 'classification' and layer_num == self.num_layers-1: 
            output_h = self.softmax(dot_product_h)  ## 1x5
          else: ## use sigmoid 
            output_h = self.sigmoid(dot_product_h)  ## 1x5
          activations.append(output_h)
          input_data = output_h

        return activations
    
    def backward_pass(self, targets, layer_activations):
        '''Executes the backpropogation algorithm.
        "targets" is the ground truth/labels
        "layer_activations" are the return value of the forward pass step
        Returns "deltas", which is a list containing weight update values for all layers (excluding the input layer of course)'''

        deltas = []
        delta_w2 = []
        if self.mode == 'classification':
          delta_y = -1* (targets - np.array(layer_activations[-1]))
          delta_w2 = delta_y.flatten()
        else:
          delta_y = (-1* (targets - np.array(layer_activations[-1]))).flatten()
          delta_z = (layer_activations[-1]*(1- layer_activations[-1])).flatten()
          for i in range(len(delta_y)):
            delta_w2.append(delta_z[i] * delta_y[i])

        deltas.append(delta_w2)

        if self.num_layers == 2:  ## only two layers exists, delta has already calculated above
          return deltas
        # for hidden layer##########################
        layer_num = self.num_layers - 2
        delta_w1 = []
        while layer_num >= 1: ## calculating deltas for all layers in backward direction
          delta_y_h = []
          delta_z_h = []
          delta_w2_trans = np.transpose(delta_w2)
          for i in range(self.weights_[layer_num].shape[0]):
            dot = []
            for j in range(delta_w2_trans.shape[0]):
              dot.append(self.weights_[layer_num][i][j] *  delta_w2_trans[j])
            delta_y_h.append(np.sum(dot))
          delta_y_h = np.array(delta_y_h).reshape((1,self.nodes_per_layer[layer_num]))
          delta_z_h = layer_activations[layer_num-1]*(1- layer_activations[layer_num-1])  ## yh1(1-yh1)
          delta_z_h = delta_z_h.flatten().reshape((1,self.nodes_per_layer[layer_num]))
          for i in range(len(delta_z_h)):
            delta_w1.append(delta_z_h[i] * delta_y_h[i])
          np.array(delta_w1).reshape((1,self.nodes_per_layer[layer_num]))
          deltas.insert(0, delta_w1)
          delta_w2 = delta_w1## back propagating
          delta_w1 = []

          layer_num = layer_num-1

        return deltas
    
    def weight_update(self, deltas, layer_inputs, lr):

        '''Executes the gradient descent algorithm.
        "deltas" is return value of the backward pass step
        "layer_inputs" is a list containing the inputs for all layers (including the input layer)
        "lr" is the learning rate'''

        for layer, nodes in enumerate(self.nodes_per_layer):
      
          if layer == 0:
            continue
          layer_inputs[layer-1] = layer_inputs[layer-1].reshape(1,self.nodes_per_layer[layer-1])
          delta_0_trans = np.transpose(deltas[layer-1]).reshape((nodes,1))
          grad1 = []
          grad2 = []
          for i in range(delta_0_trans.shape[0]):
            dot =[]
            for j in range(layer_inputs[layer-1].shape[1]):
              dot.append(delta_0_trans[i][0] * layer_inputs[layer-1][0][j])
            grad1.append(dot)
          grad1 = np.array(grad1).T.reshape((self.nodes_per_layer[layer-1], nodes))
          
          self.weights_[layer-1] = self.weights_[layer-1] - (lr*grad1)
          self.biases_[layer-1] = self.biases_[layer-1] - (lr*np.sum(deltas[layer-1]))
    
    def fit(self, Xs, Ys, epochs, lr=1e-3):
        '''Trains the model on the given dataset for "epoch" number of itterations with step size="lr". 
        Returns list containing loss for each epoch.'''
        history = []
        for epoch in tqdm_notebook(range(epochs)):
            print('epochs # : ', epoch)
            num_samples = Xs.shape[0]
            for i in range(num_samples):
                sample_input = Xs[i,:].reshape((1,self.input_shape))  ## 1x2 
                sample_target = Ys[i,:].reshape((1,self.output_shape))  ## 1x2
                activations =  self.forward_pass(sample_input) # Call forward_pass function  
                deltas =  self.backward_pass(sample_target, activations) # Call backward_pass function  
                layer_inputs = [sample_input] + activations[:-1]
                self.weight_update(deltas, layer_inputs, lr)
                 
            preds = self.predict(Xs) # Call predict function 
            if self.mode == 'regression':
                current_loss = self.mean_squared_error(preds, Ys)
            elif self.mode == 'classification':
                current_loss = self.cross_entropy_loss(preds, Ys)
            history.append(current_loss)
        return history
    
    def predict(self, Xs):
        '''Returns the model predictions (output of the last layer) for the given "Xs".'''
        predictions = []
        num_samples = Xs.shape[0]
        for i in range(num_samples):
            sample = Xs[i,:].reshape((1,self.input_shape))
            sample_prediction = self.forward_pass(sample)[-1]
            predictions.append(sample_prediction.reshape((self.output_shape,)))
        return np.array(predictions)
    
    def evaluate(self, Xs, Ys):
        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''
        pred = self.predict(Xs)
        if self.mode == 'regression':
            return self.mean_squared_error(pred, Ys)
        elif self.mode == 'classification':
            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))
    
    def plot_model(self, filename):
        '''Provide the "filename" as a string including file extension. Creates an image showing the model as a graph.'''
        graph = pydot.Dot(graph_type='digraph')
        graph.set_rankdir('LR')
        graph.set_node_defaults(shape='circle', fontsize=0)
        for i in range(self.num_layers-1):
            for n1 in range(self.nodes_per_layer[i]):
                for n2 in range(self.nodes_per_layer[i+1]):
                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')
                    graph.add_edge(edge)
        graph.write_png(filename)

nn = NeuralNetwork([2,5,6,7,6,5,2], 'classification')
nn.plot_model('graph.png')
Image('graph.png')

dataset = pd.DataFrame({
    'var1':   [0, 0, 1, 1],
    'var2':   [0, 1, 0, 1],
    # 'var3':   [0, 1, 1, 0],
    'output': [0, 1, 1, 0],
})
dataset = pd.get_dummies(dataset, columns=['output'])
dataset['output'] = pd.Series([0, 1, 1, 0])

# # starting time
start = time.time()
nn = NeuralNetwork([2,5,6,7,6,5,2], 'classification')
history = nn.fit(dataset[['var1','var2']].values, dataset[['output_0','output_1']].values, epochs=400, lr=0.005)

# nn = NeuralNetwork([2,5,6,7,6,5,2], 'regression')
# history = nn.fit(dataset[['var1','var2']].values, dataset[['output_0','output_1']].values, epochs=2000, lr=0.009)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));
# end time
end = time.time()
print("Runtime of the algorithm is ",round((end - start),3)," seconds")

"""## Task 4

You should copy your code from task3 as a starting point. You have to modify the forward and backward passes to use a different activation function; the Rectified Linear Unit (ReLu). You can look at the research papers or articles online for derivatives and other explanations. 

In this task you need to implement the activation function and its derivative yourself. You can use a separate function for derivative of activation function or include the expression in the derivative calculation. For classification, we will use softmax for output layer.
"""

class NeuralNetwork():
    @staticmethod
    def mean_squared_error(y_pred, y_true):
        # implement mean_squared_error function
        #TO DO
        return (1/len(y_true)) * (np.sum((y_true - y_pred) ** 2))

    @staticmethod
    def cross_entropy_loss(y_pred, y_true):
        # implement cross_entropy_loss function
        #TO DO

        loss = -1 * np.sum(np.multiply(np.log(y_pred), y_true), axis=1)
        return np.sum(loss) ### adding losses for all examples
    
    @staticmethod
    def accuracy(y_pred, y_true):
        # implement accuracy function
        correct = 0
        for i in range(len(y_pred)):
          if y_pred[i] == y_pred[i]:
            correct = correct + 1
        return (correct / len(y_pred)) *100    

    @staticmethod
    def relu(x):
        # implement relu function
        #TO DO
        x = x.flatten()
        output = []
        for i in x:
          if i <= 0:
            output.append(0)
          else:
            output.append(i)
        return output
    
    @staticmethod
    def relu_der(x):
        # implement relu_der function
        #TO DO
        output = []
        for i in x:
          if i <= 0:
            output.append(0)
          else:
            output.append(1)
        return np.array(output)

    @staticmethod
    def softmax(x):
        # implement softmax function
        #TO DO
        return np.exp(x) / np.sum(np.exp(x))

    @staticmethod
    def sigmoid(x):
        #TO DO
        return 1 / (1 + np.exp(-x))
    
    def __init__(self, nodes_per_layer, mode):
        '''Creates a Feed-Forward Neural Network.
        "nodes_per_layer" is a list containing number of nodes in each layer (including input layer)
        "mode" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''
        if len(nodes_per_layer) < 2:
            raise ValueError('Network must have atleast 2 layers (input and output).')
        if not (np.array(nodes_per_layer) > 0).all():
            raise ValueError('Number of nodes in all layers must be positive.')
        if mode not in ['classification','regression']:
            raise ValueError('Only "classification" and "regression" modes are supported.')
        
        self.num_layers = len(nodes_per_layer) # includes input layer
        self.nodes_per_layer = nodes_per_layer
        self.input_shape = nodes_per_layer[0]
        self.output_shape = nodes_per_layer[-1]
        self.mode = mode
        
        
        self.__init_weights(nodes_per_layer)
    
    def __init_weights(self, nodes_per_layer):
        '''Initializes all weights based on standard normal distribution and all biases to 0.'''
        self.weights_ = []
        self.biases_ = []

        for i,_ in enumerate(nodes_per_layer):
            if i == 0:
                # skip input layer, it does not have weights/bias
                continue
            
            weight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))
            self.weights_.append(weight_matrix)
            bias_vector = np.zeros(shape=(1,))
            self.biases_.append(bias_vector)
    
    def forward_pass(self, input_data):
        '''Executes the feed forward algorithm.
        "input_data" is the input to the network in row-major form
        Returns "activations", which is a list of all layer outputs (excluding input layer of course)'''
        input_data = input_data.flatten()
        activations = []

        for layer_num,nodes in enumerate(self.nodes_per_layer):
          if layer_num == 0:
                # skip input layer, it does not have weights/bias
              continue
        
          dot_product_h = []
          for i in range(self.weights_[layer_num-1].shape[1]):
            dot = []
            for j in range(len(input_data)):
              dot.append(input_data[j] * self.weights_[layer_num-1][j][i])
            dot_product_h.append(np.sum(dot) + self.biases_[layer_num-1])
          dot_product_h = np.array(dot_product_h).reshape((1, nodes))
          if self.mode == 'classification' and layer_num == self.num_layers-1: ## for output layer, if classification, use softmax
            output_h = self.softmax(dot_product_h)
          else: ## use relu
            output_h = self.relu(dot_product_h)
          output_h = np.array(output_h)
          activations.append(output_h)
          input_data = output_h ## forward propagating
    
        return activations
    
    def backward_pass(self, targets, layer_activations):
        '''Executes the backpropogation algorithm.
        "targets" is the ground truth/labels
        "layer_activations" are the return value of the forward pass step
        Returns "deltas", which is a list containing weight update values for all layers (excluding the input layer of course)'''
        deltas = []
        delta_w2 = []
        if self.mode == 'classification':
          delta_y = -1* (targets - np.array(layer_activations[-1]))  ## -(t - y)   
          delta_w2 = delta_y.flatten()
        else:
          delta_y = (-1* (targets - np.array(layer_activations[-1]))).flatten()  ## -(t - y) 
          delta_z = self.relu_der(layer_activations[-1]) ## derviative of relu
          for i in range(len(delta_y)):
            delta_w2.append(delta_z[i] * delta_y[i])

        deltas.append(delta_w2)

        if self.num_layers == 2:  ## only two layers exists, delta has already calculated above
          return deltas

        #calculation for hidden layer#
        layer_num = self.num_layers - 2
        delta_w1 = []
        while layer_num >= 1:
          delta_y_h = []
          delta_z_h = []
          delta_w2_trans = np.transpose(delta_w2)
          for i in range(self.weights_[layer_num].shape[0]):
            dot = []
            for j in range(delta_w2_trans.shape[0]):
              dot.append(self.weights_[layer_num][i][j] *  delta_w2_trans[j])
            delta_y_h.append(np.sum(dot))
          delta_y_h = np.array(delta_y_h).reshape((1,self.nodes_per_layer[layer_num]))
          delta_z_h = self.relu_der(layer_activations[layer_num-1])
          delta_z_h = delta_z_h.flatten().reshape((1,self.nodes_per_layer[layer_num]))
          for i in range(len(delta_z_h)):
            delta_w1.append(delta_z_h[i] * delta_y_h[i])
          np.array(delta_w1).reshape((1,self.nodes_per_layer[layer_num]))
          deltas.insert(0, delta_w1)
          delta_w2 = delta_w1## back propagating
          delta_w1 = []

          layer_num = layer_num-1

        
        return deltas

    def weight_update(self, deltas, layer_inputs, lr):
        '''Executes the gradient descent algorithm.
        "deltas" is return value of the backward pass step
        "layer_inputs" is a list containing the inputs for all layers (including the input layer)
        "lr" is the learning rate'''
        for layer, nodes in enumerate(self.nodes_per_layer):
      
          if layer == 0:
            continue
          layer_inputs[layer-1] = np.array(layer_inputs[layer-1]).reshape(1,self.nodes_per_layer[layer-1])
          delta_0_trans = np.transpose(deltas[layer-1]).reshape((nodes,1))
          grad1 = []
          for i in range(delta_0_trans.shape[0]):
            dot =[]
            for j in range(layer_inputs[layer-1].shape[1]):
              dot.append(delta_0_trans[i][0] * layer_inputs[layer-1][0][j])
            grad1.append(dot)
          grad1 = np.array(grad1).T.reshape((self.nodes_per_layer[layer-1], nodes))
          
          self.weights_[layer-1] = self.weights_[layer-1] - (lr*grad1)
          self.biases_[layer-1] = self.biases_[layer-1] - (lr*np.sum(deltas[layer-1]))
            
    def fit(self, Xs, Ys, epochs, lr=1e-3):
        '''Trains the model on the given dataset for "epoch" number of itterations with step size="lr". 
        Returns list containing loss for each epoch.'''
        history = []
        # print(Xs.shape)
        # print(Ys.shape)
        for epoch in tqdm_notebook(range(epochs)):
            print('epoch  : ', epoch)
            num_samples = Xs.shape[0]
            for i in range(num_samples):
                sample_input = Xs[i,:].reshape((1,self.input_shape))
                sample_target = Ys[i,:].reshape((1,self.output_shape))
                
                activations =  self.forward_pass(sample_input) # Call forward_pass function 

                deltas =  self.backward_pass(sample_target, activations) # Call backward_pass function  

                layer_inputs = [sample_input] + activations[:-1]
                self.weight_update(deltas, layer_inputs, lr)
                 
            preds = self.predict(Xs) # Call predict function 
            if self.mode == 'regression':
                current_loss = self.mean_squared_error(preds, Ys)
            elif self.mode == 'classification':
                current_loss = self.cross_entropy_loss(preds, Ys)
            history.append(current_loss)
        return history
    
    def predict(self, Xs):
        '''Returns the model predictions (output of the last layer) for the given "Xs".'''
        predictions = []
        num_samples = Xs.shape[0]
        for i in range(num_samples):
            sample = Xs[i,:].reshape((1,self.input_shape))
            sample_prediction = self.forward_pass(sample)[-1]
            predictions.append(sample_prediction.reshape((self.output_shape,)))
        return np.array(predictions)
    
    def evaluate(self, Xs, Ys):
        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''
        pred = self.predict(Xs)
        if self.mode == 'regression':
            return self.mean_squared_error(pred, Ys)
        elif self.mode == 'classification':
            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))
    
    def plot_model(self, filename):
        '''Provide the "filename" as a string including file extension. Creates an image showing the model as a graph.'''
        graph = pydot.Dot(graph_type='digraph')
        graph.set_rankdir('LR')
        graph.set_node_defaults(shape='circle', fontsize=0)
        for i in range(self.num_layers-1):
            for n1 in range(self.nodes_per_layer[i]):
                for n2 in range(self.nodes_per_layer[i+1]):
                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')
                    graph.add_edge(edge)
        graph.write_png(filename)

nn = NeuralNetwork([1,10,1], 'regression')
nn.plot_model('graph.png')
Image('graph.png')

reg = LinearRegression()
reg = reg.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));

d = np.arange(-1.5, 2.5, 0.1).reshape((40,1))
preds = reg.predict(d)


fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Linear Regression {}'.format(rollnumber));
ax.legend();

# starting time
start = time.time()

nn = NeuralNetwork([1,10,1], 'regression')
history = nn.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)), epochs=40, lr=0.009)
preds = nn.predict(d)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));
# end time
end = time.time()
print("Runtime of the algorithm is ",round((end - start),3)," seconds")

fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Neural Network Regression {}'.format(rollnumber));
ax.legend();

"""## Task 5

Again, if it helps copy only your code from Task3 as a starting point in the next cell. To cap off this assignment you will fully vectorize your implementation. This means changing the primary functions again. There will be a handout on LMS to explain this further.

After you do this, the runtime of the `fit` function will just be $O(e)$ (again, assuming all functions called in the loop take constant time).
"""

class NeuralNetwork():
    @staticmethod
    def mean_squared_error(y_pred, y_true):
        # implement mean_squared_error function
        #TO DO
        return (1/len(y_true)) * (np.sum((y_true - y_pred) ** 2))

    @staticmethod
    def cross_entropy_loss(y_pred, y_true):
        # implement cross_entropy_loss function
        #TO DO

        loss = -1 * np.sum(np.multiply(np.log(y_pred), y_true), axis=1) ## 60000 x 1
        return np.sum(loss) ### adding losses for all examples
    
    @staticmethod
    def accuracy(y_pred, y_true):
        # implement accuracy function
        correct = 0
        for i in range(len(y_pred)):
          if y_pred[i] == y_pred[i]:
            correct = correct + 1
        return (correct / len(y_pred)) *100

    @staticmethod
    def softmax(x):
        # implement softmax function
        #TO DO
        return np.exp(x) / np.sum(np.exp(x))

    @staticmethod
    def sigmoid(x):
        #TO DO
        return 1 / (1 + np.exp(-x))
        
    
    def __init__(self, nodes_per_layer, mode,confusion_matrix=False):
        '''Creates a Feed-Forward Neural Network.
        "nodes_per_layer" is a list containing number of nodes in each layer (including input layer)
        "mode" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''
        if len(nodes_per_layer) < 2:
            raise ValueError('Network must have atleast 2 layers (input and output).')
        if not (np.array(nodes_per_layer) > 0).all():
            raise ValueError('Number of nodes in all layers must be positive.')
        if mode not in ['classification','regression']:
            raise ValueError('Only "classification" and "regression" modes are supported.')
        
        self.num_layers = len(nodes_per_layer) # includes input layer
        self.nodes_per_layer = nodes_per_layer
        self.input_shape = nodes_per_layer[0]
        self.output_shape = nodes_per_layer[-1]
        self.mode = mode
        self.confusion_matrix=confusion_matrix 
        
        self.__init_weights(nodes_per_layer)
    
    def __init_weights(self, nodes_per_layer):
        '''Initializes all weights based on standard normal distribution and all biases to 0.'''
        self.weights_ = []
        self.biases_ = []
        for i,_ in enumerate(nodes_per_layer):
            if i == 0:
                # skip input layer, it does not have weights/bias
                continue
            
            weight_matrix = np.random.normal(size=(nodes_per_layer[i-1], nodes_per_layer[i]))
            self.weights_.append(weight_matrix)
            bias_vector = np.zeros(shape=(1,))
            self.biases_.append(bias_vector)
    

    def forward_pass(self, input_data):
        '''Executes the feed forward algorithm.
        "input_data" is the input to the network in row-major form
        Returns "activations", which is a list of all layer outputs (excluding input layer of course)'''
        
        activations = []
        for layer_num in range(self.num_layers):
          if layer_num == 0: ## ignore input layer
            continue
          dot_product_h = np.dot(input_data, self.weights_[layer_num-1])
          dot_product_h = dot_product_h + self.biases_[layer_num-1]
          if layer_num == self.num_layers-1 and self.mode == 'classification':
            output_h = self.softmax(dot_product_h)
          else:
            output_h = self.sigmoid(dot_product_h)
          activations.append(output_h)
          input_data = output_h

        return activations
    
    def backward_pass(self, targets, layer_activations):
        '''Executes the backpropogation algorithm.
        "targets" is the ground truth/labels
        "layer_activations" are the return value of the forward pass step
        Returns "deltas", which is a list containing weight update values for all layers (excluding the input layer of course)'''
        deltas = []
        delta_w2 = []
        if self.mode == 'classification':
          delta_y = -1* (targets - np.array(layer_activations[-1]))  ## -(t - y)
          delta_w2 = delta_y.flatten()
        else:
          delta_y = (-1* (targets - np.array(layer_activations[-1]))).flatten()  ## -(t - y)
          delta_z = (layer_activations[-1]*(1- layer_activations[-1])).flatten()  ## y01(1-y01)
          delta_w2 = np.multiply(delta_y, delta_z)

        deltas.append(delta_w2)

        if self.num_layers == 2:  ## only two layers exists, delta has already calculated above
          return deltas
        # for hidden layer##
        layer_num = self.num_layers - 2
        while layer_num >= 1:
          delta_y_h = np.transpose(np.dot(self.weights_[layer_num] , np.transpose(delta_w2)))
          delta_z_h = layer_activations[layer_num-1]*(1- layer_activations[layer_num-1])  ## yh1(1-yh1)
          delta_z_h = delta_z_h.flatten()
          delta_w1 = np.multiply(delta_y_h, delta_z_h)
          deltas.insert(0, delta_w1)
          delta_w2 = delta_w1## back propagating
          layer_num = layer_num-1

        return deltas

    def weight_update(self, deltas, layer_inputs, lr):
        '''Executes the gradient descent algorithm.
        "deltas" is return value of the backward pass step
        "layer_inputs" is a list containing the inputs for all layers (including the input layer)
        "lr" is the learning rate'''
        for layer, nodes in enumerate(self.nodes_per_layer):
      
          if layer == 0:
            continue
          layer_inputs[layer-1] = np.array(layer_inputs[layer-1]).reshape(1,self.nodes_per_layer[layer-1])
          grad1= np.transpose(np.dot(np.transpose(deltas[layer-1]).reshape((nodes,1)), layer_inputs[layer-1].reshape(1,self.nodes_per_layer[layer-1]))) 
          
          self.weights_[layer-1] = self.weights_[layer-1] - (lr*grad1)
          self.biases_[layer-1] = self.biases_[layer-1] - (lr*np.sum(deltas[layer-1]))
        

    
    def fit(self, Xs, Ys, epochs, lr=1e-3):
        '''Trains the model on the given dataset for "epoch" number of itterations with step size="lr". 
        Returns list containing loss for each epoch.'''
        history = []
        print(Xs.shape)
        print(Ys.shape)
        for epoch in tqdm_notebook(range(epochs)):
            print('epoch #', epoch)
            num_samples = Xs.shape[0]
            for i in range(num_samples):
                sample_input = Xs[i,:].reshape((1,self.input_shape)) ## 1x784
                sample_target = Ys[i,:].reshape((1,self.output_shape))
                
                activations =  self.forward_pass(sample_input) # Call forward_pass function 
                deltas =  self.backward_pass(sample_target, activations) # Call backward_pass function  
                layer_inputs = [sample_input] + activations[:-1]
                self.weight_update(deltas, layer_inputs, lr)
            preds = self.predict(Xs) # Call predict function 
            if self.mode == 'regression':
                current_loss = self.mean_squared_error(preds, Ys)
            elif self.mode == 'classification':
                current_loss = self.cross_entropy_loss(preds, Ys)
            if  (epoch==epochs-1) and (self.mode == 'classification') and (self.confusion_matrix==True):
                  confusion_mat=confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1))  
                  plot_confusion_matrix(confusion_mat)
                  report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1)), target_names=class_labels)
                  print(report)
            history.append(current_loss)
        return history
    
    def predict(self, Xs):
        '''Returns the model predictions (output of the last layer) for the given "Xs".'''
        predictions = []
        num_samples = Xs.shape[0]
        for i in range(num_samples):
            sample = Xs[i,:].reshape((1,self.input_shape))
            sample_prediction = self.forward_pass(sample)[-1]
            predictions.append(sample_prediction.reshape((self.output_shape,)))
        return np.array(predictions)
    
    def evaluate(self, Xs, Ys):
        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''
        pred = self.predict(Xs)
        if self.mode == 'regression':
            return self.mean_squared_error(pred, Ys)
        elif self.mode == 'classification':
            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))
    
    def plot_model(self, filename):
        '''Provide the "filename" as a string including file extension. Creates an image showing the model as a graph.'''
        graph = pydot.Dot(graph_type='digraph')
        graph.set_rankdir('LR')
        graph.set_node_defaults(shape='circle', fontsize=0)
        for i in range(self.num_layers-1):
            for n1 in range(self.nodes_per_layer[i]):
                for n2 in range(self.nodes_per_layer[i+1]):
                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')
                    graph.add_edge(edge)
        graph.write_png(filename)

"""### Classification for Task5

You will repeat the classfication task on Fashion Mnist dataset. You can change the hidden layers and their sizes to acheive the optimal results. We have passed the confusion_matrix=True for this classification task only.
"""

# starting time
start = time.time()
# Define the input size and output size of  Fashion MNIST dataset

# You can change the number of hidden layers and their sizes  to get the optimal results

nn = NeuralNetwork(nodes_per_layer=[784,70,10],mode='classification',confusion_matrix=True)
# You can tweak the learning rate and epochs to get the optimal results
history = nn.fit(X_train, y_train, epochs=7, lr=0.0005)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));
# end time
end = time.time()
print("Runtime of the algorithm is ",round((end - start),3)," seconds")

"""### Regression for Task5"""

reg = LinearRegression()
reg = reg.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
d = np.arange(-1.5, 2.5, 0.1).reshape((40,1))
preds = reg.predict(d)

fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Linear Regression {}'.format(rollnumber));
ax.legend();

# starting time
start = time.time()

nn = NeuralNetwork([1,10,20,10,1], 'regression')
history = nn.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)), epochs=130, lr=1e-2)
preds = nn.predict(d)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));
# end time
end = time.time()
print("Runtime of the algorithm is ",round((end - start),3)," seconds")

fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Neural Network Regression {}'.format(rollnumber));
ax.legend();

"""## Dummy Regression Task"""

a = np.random.uniform(low=0.0, high=0.5, size=(150,))
b = np.random.uniform(low=0.0, high=0.5, size=(150,))
dataset = pd.DataFrame({
    'var1':   a,
    'var2':   b,
    'output': a+b,
})
print(dataset.shape)
dataset.head()

nn = NeuralNetwork([2,3,5,1], 'regression')
nn.plot_model('graph.png')
Image('graph.png')

history = nn.fit(dataset[['var1','var2']].values, dataset[['output']].values, epochs=2000, lr=0.001)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));

test_data = np.array([[0.4,0.1],
                      [0.2,0.3]])
nn.predict(test_data)

"""## Dummy Classification Task"""

# XOR logic operator
dataset = pd.DataFrame({
    'var1':   [0, 0, 1, 1],
    'var2':   [0, 1, 0, 1],
    'output': [0, 1, 1, 0],
})
dataset = pd.get_dummies(dataset, columns=['output'])
dataset['output'] = pd.Series([0, 1, 1, 0])
print(dataset.shape)
dataset.head()
# The columns 'output_0' and 'output_1' are one-hot encoded representation of the categorical column 'output'

nn = NeuralNetwork([2,5,2], 'classification')
nn.plot_model('graph.png')
Image('graph.png')

history = nn.fit(dataset[['var1','var2']].values, dataset[['output_0','output_1']].values, epochs=3000, lr=0.01)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));

nn.predict(dataset[['var1','var2']].values).argmax(axis=1) == dataset[['output_0','output_1']].values.argmax(axis=1)

"""## Regression Demo

Code for Demos adapted from tutorial 1, refer to it if you need a refresher (available on LMS)
"""

data_x, _ = make_moons(200, noise=0.18)
plt.scatter(data_x[:,0], data_x[:,1]);

reg = LinearRegression()
reg = reg.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));

d = np.arange(-1.5, 2.5, 0.1).reshape((40,1))
preds = reg.predict(d)

fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Linear Regression {}'.format(rollnumber));
ax.legend();

nn = NeuralNetwork([1,10,20,10,1], 'regression')
history = nn.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)), epochs=20000, lr=1e-4)
preds = nn.predict(d)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));

fig, ax = plt.subplots()
ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));
ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');
ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Neural Network Regression {}'.format(rollnumber));
ax.legend();

"""## Classification Demo"""

# Helper function to plot a decision boundary.
# If you don't fully understand this function don't worry
def plot_decision_boundary(pred_func, x_min, x_max, y_min, y_max, cmap, ax):
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.flatten(), yy.flatten()])
    Z = Z.reshape(xx.shape)
    # Plot the contour
    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)

data_x, data_y = make_moons(200, noise=0.20)
plt.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);
plt.gca().set(xlabel='Feature 1', ylabel='Feature 2');

clf = LogisticRegression(solver='lbfgs')
clf = clf.fit(data_x, data_y);

fig, ax = plt.subplots()
x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5
y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5
plot_decision_boundary(lambda x: clf.predict(x), 
                       x_min, x_max, y_min, y_max, 
                       plt.cm.Spectral, ax)
ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);
ax.set(xlabel='Feature 1', ylabel='Feature 2', title='Logistic Regression Classifier {}'.format(rollnumber));

nn = NeuralNetwork([2,10,10,2], 'classification')
history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=2000, lr=1e-3)
plt.plot(history);
plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));

fig, ax = plt.subplots()
x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5
y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5
plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), 
                       x_min, x_max, y_min, y_max, 
                       plt.cm.Spectral, ax)
ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);
ax.set(xlabel='Feature 1', ylabel='Feature 2', title='Neural Network Classifier {}'.format(rollnumber));