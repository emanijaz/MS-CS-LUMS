{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS437_CS5317_HW04_PARTB_VAE.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pnb4O39J_0ZL"},"source":["### VAE as Generative Model\n","\n","In this part we will draw a comparison between Autoencoders and Variational Autoencoders. We will see how plain AE are good to learn a single latent representation of data but they are not good for generation when we sample from proability distribtion. On the other hand, VAE can map input data to continuous probability distribution. This distribution is normal distribution in case of VAEs. Once we learn this mapping, we can generate new images by simply taking samples from the learnt distribution. "]},{"cell_type":"markdown","metadata":{"id":"8bgUWwLUHa2o"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"rR1Xd-d7vpBZ"},"source":["import os\r\n","import random\r\n","import numpy as np\r\n","from glob import glob\r\n","from scipy.stats import norm\r\n","import matplotlib.pyplot as plt\r\n","from mpl_toolkits.axes_grid1 import ImageGrid\r\n","\r\n","import tensorflow as tf\r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\r\n","from keras.layers import Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\r\n","from keras.models import Model\r\n","from keras import backend as K\r\n","from keras.optimizers import Adam\r\n","from keras.callbacks import ModelCheckpoint \r\n","from keras.utils import plot_model\r\n","\r\n","# just a little hack so that tensorflow can accpet our custom loss function\r\n","from tensorflow.python.framework.ops import disable_eager_execution\r\n","disable_eager_execution()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"td7QUAQelTFw"},"source":["### Loading, Unzipping and Displaying the Dataset"]},{"cell_type":"markdown","metadata":{"id":"b21_0FHGsY-S"},"source":["Mounting your google drive."]},{"cell_type":"code","metadata":{"id":"iMOtjPOsM3vg"},"source":["from google.colab import drive\r\n","drive.mount('/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NeafZrkgsgBs"},"source":["Unzipping the data file to load it locally in the colab runtime. You can see your unzipped files by clicking the folder icon on left side of your colab."]},{"cell_type":"code","metadata":{"id":"NOeX1b3k2NfY"},"source":["# replace this your google drive path of the zip file of dataset provided with this homework\r\n","!unzip -o -q \"/drive/MyDrive/CS5317_DeepLearning_SP21/Assign04/ffhq-dataset.zip\" -d \"/content/data/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1ur9Fq2OG3v"},"source":["DATA_FOLDER = '/content/data/'\n","\n","filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.png')))\n","NUM_IMAGES = len(filenames)\n","print(\"Total number of images : \" + str(NUM_IMAGES))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAOEfLF0Oi4N"},"source":["The dataset is quite large (70000 images) which makes it impossible to load it all at the same time in computer memory. We will use Keras' <i>ImageDataGenerator</i> object and call its member function - <i>flow_from_directory</i> to define the flow of data directly from disk rather than loading the entire dataset into memory. You can also apply various transformations (for augmentation) to the images directly while loading the data (e.g normalizing, rescaling, rotating etc).\n","\n","\n","You can read more about this in official Keras [documentation](https://keras.io/preprocessing/image/#flow_from_directory).\n","\n","Below we have setup everything you need for this dataset. You will need to pass <i>data_flow</i> object to <i>fit</i> functions later when training your model."]},{"cell_type":"code","metadata":{"id":"-I53l3rAPqyE"},"source":["INPUT_DIM = (128,128,3) # Image dimension\n","BATCH_SIZE = 512        # batch of images returned by ImageDataGenerator\n","Z_DIM = None            # Dimension of the latent vector (z) [Specify your latent_vector dimension here]\n","\n","data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, target_size = INPUT_DIM[:2], \n","                                                                   batch_size = BATCH_SIZE, shuffle = True, \n","                                                                   class_mode = 'input', subset = 'training')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"748HAEQnw7qO"},"source":["Utility function to display grid of images."]},{"cell_type":"code","metadata":{"id":"w-hh7h1Pt6q9"},"source":["def display_image_grid(images, num_rows, num_cols, title_text):\n","\n","    fig = plt.figure(figsize=(num_cols*3., num_rows*3.), )\n","    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.15)\n","\n","    for ax, im in zip(grid, images):\n","        ax.imshow(im)\n","        ax.axis(\"off\")\n","    \n","    plt.suptitle(title_text, fontsize=20)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_qH5HXEtld2"},"source":["Displaying some samples images from the dataset."]},{"cell_type":"code","metadata":{"id":"mLq_Q6o5mnC6"},"source":["# a batch of 512 images returned by data generator\n","sample_images = next(data_flow)[0] \n","\n","# only taking 10 of those to display\n","sample_images = sample_images[:10]\n","\n","# displaying the images\n","display_image_grid(sample_images, 2, 5, \"Some sample Images from ffhq-dataset\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IHNXkqbQSPab"},"source":["## AUTOENCODER"]},{"cell_type":"markdown","metadata":{"id":"bZhwpjUmgDxq"},"source":["#### Encoder\n","\n","Below you will create the model for your encoder just like the one in Image Completion task (but not necessarily that same). The architecture of the Encoder consists of a stack of convolutional layers followed by a dense (fully connected) layer which outputs a vector of size <i>Z_DIM</i>. The whole image of size 128x128x3 is decoed into this latent space vector of size <i>Z_DIM</i>.\n","\n","\n","NOTE: You can experiment with the number of feature maps, kernel size, strides and number of conv layer."]},{"cell_type":"code","metadata":{"id":"8XlsuP_NeNWb"},"source":["ae_encoder = None\n","ae_encoder_input = None\n","ae_encoder_output = None\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","ae_encoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYC79JgkjCYO"},"source":["#### Decoder\n","\n","Just like the encoder you will create the model for the decoder. This model can be the exact mirror of encoder model, but that is not mandatory.\n","\n","Since the function of the Decoder to reconstruct the image from the latent vector. Therefore, it is necessary to define the decoder so as to increase the size of the activations gradually through the network. This can be achieved through the  [Conv2DTransponse](https://keras.io/layers/convolutional/#conv2dtranspose) layer. This layer produces an output tensor double the size of the input tensor in both height and width.\n","\n","Again, you can experiment with the number of feature maps, kernel size, strides and number of conv layer.\n"]},{"cell_type":"code","metadata":{"id":"TvBVuQvMjF3A"},"source":["######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","ae_decoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-lK7PJGCyoa9"},"source":["#### Attaching the Decoder to the Encoder\n","\n","Finally, here we connect the encoder to the docoder."]},{"cell_type":"code","metadata":{"id":"ma7ZQ2pow2am"},"source":["autoencoder_model = None\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","# The input of the autoencoder will be the same as of encoder\n","\n","# The output of the autoencoder will be the output of decoder, when passed encoder input\n","\n","\n","# Input to the combined model will be the input to the encoder.\n","# Output of the combined model will be the output of the decoder.\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","autoencoder_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29JoCbVi5PJO"},"source":["### Training the AE\n","\n","The hyperparameters are the same as given in the Image Completion task."]},{"cell_type":"markdown","metadata":{"id":"Qsa9Q7Fn6eyP"},"source":["Also for training, you can use <i>Adam</i> optimizer with the learning rate given below (or you can try out your own).\n","\n","The number of epochs given are 10, but experiment with that number to know where you can acheive the best results."]},{"cell_type":"code","metadata":{"id":"JdGJw4Fb6I7e"},"source":["LEARNING_RATE = 0.0005\n","N_EPOCHS = 10\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","# compile your model here\n","\n","########################### END OF YOUR CODE ##########################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_mJeKcn57JQA"},"source":["Now simply call the <i>fit</i> function of the model with the appropriate paramters."]},{"cell_type":"code","metadata":{"id":"1F7g3wxz6Mkp"},"source":["######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbFnCBjKu9xa"},"source":["### Reconstruction\n","\n","Now we will get a batch of images from ImageDataGenerator object and try to reconstruct the images after passing it through our autoencoder.\n","\n","The first image grid shows the original images and the second grid shows the reconstructed images after passing it through the AE."]},{"cell_type":"code","metadata":{"id":"ft5zuMQ6CpyA"},"source":["test_batch = next(data_flow)[0]\n","test_images = test_batch[:10]\n","\n","reconst_images = autoencoder_model.predict(test_images)\n","\n","display_image_grid(test_images, 2, 5, \"Original Images\")\n","display_image_grid(reconst_images, 2, 5, \"Reconstructed Images with Autoencoder\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6F0h04x9_XQ"},"source":["<i>NOTE:</i> The reason that you are seeing the reconstructed images as blurry because MSE averages out the differences between individual pixel values. GANs (which you will see in the next assignment) on the other hand produces much sharper results. "]},{"cell_type":"markdown","metadata":{"id":"j4ZHW0pKWJse"},"source":["Adding noise before decoding the images"]},{"cell_type":"code","metadata":{"id":"4k8dwk3i_LbU"},"source":["num_of_images = 10\n","\n","# encoding our images\n","encodings = ae_encoder.predict(test_images)\n","\n","# adding random normal noise to the encoded latent vectors\n","encodings += np.random.normal(0.0, 1.0, size = (num_of_images, Z_DIM))\n","\n","# reconstruct from noisy latent vector\n","reconst_images_noisy = ae_decoder.predict(encodings)\n","\n","display_image_grid(reconst_images_noisy, 2, 5, \"Reconstructed Images from Noisy Latent Vector\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uLbdXT_rWiw8"},"source":["It can be observed that the images are starting to get distorted with a bit of noise added to its encodings. One possible reason could be that the model did not ensure that the space around the encoded values (latent space) was continuous. We will see later how to overcome this with the help of Variational Autoencoder."]},{"cell_type":"markdown","metadata":{"id":"5m9arqxuct41"},"source":["### Generation \n","Generate images from latent vectors sampled from a standard normal distribution"]},{"cell_type":"code","metadata":{"id":"6lVD_ABySdVK"},"source":["reconst_images = ae_decoder.predict(np.random.normal(0,1,size=(num_of_images, Z_DIM)))\n","\n","display_image_grid(reconst_images, 2, 5, \"Images generated by sampling from normal distribution\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0XnSuVPSfIuW"},"source":["It is evident that the latent vector sampled from a standard normal distribution can not be used to generate new faces. This shows that the latent vectors generated by the model are not centered/symmetrical around the origin. This also strengthens our inference that the latent space is not continuous.\n","\n","Since we do not have a definite distribution to sample latent vectors from, it is unclear as to how we can generate new faces. We observed that adding a bit of noise to the latent vector does not produce new faces. We can encode and decode images but that does not meet our objective of learning the joing distribution of data. \n","\n","Building on this thought, wouldn't it be great if we could generate new faces from latent vectors sampled from a standard normal distribution? This is essentially what a Variational Autoencoder does."]},{"cell_type":"markdown","metadata":{"id":"0aWwM1tYjY3h"},"source":["## VARIATIONAL AUTOENCODER\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e0KLT5byj_Z4"},"source":["Variational Autencoders tackle most of the problems discussed above. They are trained to generate new faces from latent vectors sampled from a standard normal distribution. While a Simple Autoencoder learns to map each image to a fixed point in the latent space, the Encoder of a Variational Autoencoder (VAE) maps each image to a z-dimensional standard normal distribution. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"MYIF0s59JX6w"},"source":["Here is a high level overview of what a Variational Autoencoder does.\n","\n","<br>\n","<img src=\"https://blog.bayeslabs.co/assets/img/vae.jpg\"> \n","<br><i>Source : blog.bayeslabs.co/2019/06/04/All-you-need-to-know-about-Vae</i>"]},{"cell_type":"markdown","metadata":{"id":"jl0sSds_F_rB"},"source":["#### Encoder\n","\n","The encoder for the Variational AE is little trickier than the simple autoencoder.  While a Simple Autoencoder learns to map each image to a fixed point in the latent space, the Encoder of a Variational Autoencoder (VAE) maps each image to a z-dimensional standard normal distribution. It will loos something like this:\n","\n","<center>\n","\n","![picture](https://drive.google.com/uc?export=view&id=1e3iGHK0s83O-RjpbRkO4OWKU52B5kS2I)\n","\n","</center>\n","\n","You will need Keras' functional API to make this type of model as this is not a simple feed forward network. You can read and learn more about Function API [here](https://keras.io/guides/functional_api/).\n","\n","The input to the Decoder, as shown in the image above is a vector sampled from the normal distribution represented by the output of the Encoder - $\\mu$ and $\\sigma$. This sampling can be done as follows:\n","\n","<center>\n","$Z = \\mu + \\sigma\\varepsilon$\n","</center>\n","\n","where $\\varepsilon$ is a sampled from a multivariate standard normal distribution.\n"]},{"cell_type":"code","metadata":{"id":"jZ5hNK4TKwTW"},"source":["mean_mu = None\n","var = None\n","vae_encoder_input = None\n","vae_encoder = None\n","vae_encoder_output = None\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","# define your convolution layers here\n","\n","\n","# define mean and var dense layers outputed by your model\n","\n","########################### END OF YOUR CODE ##########################\n","\n","# here is your model outputing mu and var seperately\n","# now we will take samples from these paramters of distributions\n","# and these samples will be our latent vector, which \n","# can be feed to our decoder\n","\n","\n","# Defining a function for sampling\n","# this function takes mean and var vectors and \n","def sample_from_distribution(args):\n","  mean_mu, var = args\n","  epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n","  return mean_mu + K.exp(var/2)*epsilon   \n","  \n","# Using a Keras Lambda Layer to include the sampling function as a layer in the model\n","vae_encoder_output = Lambda(sample_from_distribution, name='encoder_output')([mean_mu, var])\n","\n","vae_encoder = Model(vae_encoder_input, vae_encoder_output)\n","vae_encoder.summary()\n","# tf.keras.utils.plot_model(vae_encoder, \"vae_encoder.png\", show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s10B4Q7USlcP"},"source":["#### Decoder\n","Since the Decoder remains the same, you can use the same architecture of the decoder of Autoencoder."]},{"cell_type":"code","metadata":{"id":"hlygbOIvNuEw"},"source":["vae_decoder_input = None\n","vae_decoder_output = None\n","vae_decoder = None\n","\n","######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","\n","vae_decoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qwpz77-9Shrz"},"source":["#### Attaching the Decoder to the Encoder\n","\n","Just like in the case of autoencoder, you will connect your encoder with docer and make the final model.\n"]},{"cell_type":"code","metadata":{"id":"uMrFtgW7TGq9"},"source":["######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","# The input to the model will be the image fed to the encoder.\n","\n","\n","# Output will be the output of the decoder. The term - decoder(encoder_output) \n","# combines the model by passing the encoder output to the input of the decoder.\n","\n","\n","# Input to the combined model will be the input to the encoder.\n","# Output of the combined model will be the output of the decoder.\n","\n","\n","########################### END OF YOUR CODE ##########################\n","\n","vae_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69jeaO2_UFLC"},"source":["### Loss Function\n","\n","The loss function is a sum of MSE and KL Divergence. MSE error contorls the quality of images (as already seen in the simple autoencoder) while including the KL divergence loss in addition to the MSE loss, the VAE is forced to ensure that the encodings are very similar to a multivariate standard normal distribution. Since a multivariate standard normal distribution has a zero mean, it is centered around the origin. Mapping each image to a standard normal distribution as opposed to a fixed point ensures that the latent space is continuous and the latent vectors are centered around the origin. Here is equation of KL loss, where <b>$\\mu$</b> and <b>$\\sigma$</b> are the vectors returned by encoder.\n","\n","<br>\n","<center>\n","$D_{KL}[N(\\mu,\\sigma) \\ || \\ N(0,1)] = \\frac{1}{2}\\sum_{i=1}^{z}(1+\\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2)$\n","</center>\n","</br>\n","\n","A weight (loss factor) is assigned to the MSE loss. This penalizes the model by loss factor more than KL Diverge to ensure that images produced are of good quality. If we make this loss factor small, the images will be not of good quality. If we make this loss factor large, then our model will simply act as a simple AE.\n","\n","Hence, this (loss_factor) is also a hyperparamter that you need to take care of. \n","\n","Here is the [link](https://keras.io/api/losses/) to Keras documentation on how to create custom loss functions. "]},{"cell_type":"code","metadata":{"id":"_Urh6rJT7Lde"},"source":["def total_loss(y_true, y_pred):\n","\n","    mean_vector = mean_mu   # mean vector outputed by encoder\n","    var_vector = var        # var vector outputed by encoder\n","    mse_loss = 0\n","    kl_loss = 0 \n","\n","    ######################## WRITE YOUR CODE BELOW ########################\n","\n","    # calculate mse loss here\n","\n","    # calculate kl loss here\n","\n","    ########################### END OF YOUR CODE ##########################\n","\n","    return LOSS_FACTOR * mse_loss + kl_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G_caxSKyBC4g"},"source":["The loss function is a sum of RMSE and KL Divergence. A weight is assigned to the RMSE loss, known as the loss factor. The loss factor is multiplied with the RMSE loss. If we use a high loss factor, the drawbacks of a Simple Autoencoder start to appear. However, if we use a loss factor too low, the quality of the reconstructed images will be poor. Hence the loss factor is a hyperparameter that needs to be tuned."]},{"cell_type":"code","metadata":{"id":"hFgWtViZWuIx"},"source":["LEARNING_RATE = None\n","N_EPOCHS = None\n","LOSS_FACTOR = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cz1alSFTsh0"},"source":["### Training the VAE\n"]},{"cell_type":"markdown","metadata":{"id":"PBKQSxBW2f9B"},"source":["Compile your model below."]},{"cell_type":"code","metadata":{"id":"-VpdQdXLU5wR"},"source":["######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vVZcyki2iM8"},"source":["Now call fit function on your model with appropriate paramters."]},{"cell_type":"code","metadata":{"id":"LIejqAfqVCoK"},"source":["######################## WRITE YOUR CODE BELOW ########################\n","\n","\n","########################### END OF YOUR CODE ##########################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MxKSLUcYC3k"},"source":["### Reconstruction\n","The reconstruction process is the same as that of the Simple Autoencoder."]},{"cell_type":"code","metadata":{"id":"ikzw8OqiYPL8"},"source":["test_batch = next(data_flow)[0]\n","test_images = test_batch[:10]\n","\n","reconst_images = vae_model.predict(test_images)\n","\n","display_image_grid(test_images, 2, 5, \"Original Images\")\n","\n","display_image_grid(reconst_images, 2, 5, \"Reconstructed Images from Variational Autoencoder\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rop_D8i0WcxH"},"source":["###Generation\n","Generating new faces from random vectors sampled from a standard normal distribution. "]},{"cell_type":"code","metadata":{"id":"Qs0pk3zdY6pK"},"source":["reconst_images = vae_decoder.predict(np.random.normal(0,1,size=(10, Z_DIM)))\n","\n","display_image_grid(reconst_images, 2, 5, \"Images generated by sampling from normal distribution\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oa7MQz0ObSSw"},"source":["The VAE is evidently capable enough of producing new faces from vectors samped from a standard normal distribution. The fact that a neural network is capable of generating new faces from random noise shows how powerful it is in performing extremely complex mappings!"]},{"cell_type":"markdown","metadata":{"id":"uuTMNIE55wXf"},"source":["## REPORT\n","\n","Report your results for different values of <b>Z_DIM</b>, <b>learning rate</b>, <b>optimizers</b>, <b> encoder and decoder model and Loss Factor</b> and tell us for which configuration you acheived the best results (The best run model should be the last run model in this notebook, showing the results in the cell above).\n","\n","Answer:"]}]}